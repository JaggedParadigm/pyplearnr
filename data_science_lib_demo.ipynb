{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Library demo\n",
    "Here I demonstrate the data science library I developed to quickly build scikit learn pipelines with optional scaling, feature interaction, data transformation (e.g. PCA, t-SNE) steps. It runs the pipeline through a grid-search (all combinations or a specific number of them) stratified (if classification) k-folds cross-validation and outputs the best model.\n",
    "\n",
    "## Titanic dataset\n",
    "Here I use the Titanic dataset I've cleaned and pickled in a separate tutorial.\n",
    "\n",
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 890 entries, 0 to 890\n",
      "Data columns (total 9 columns):\n",
      "Survived    890 non-null int64\n",
      "Pclass      890 non-null int64\n",
      "Sex         890 non-null object\n",
      "Age         890 non-null float64\n",
      "SibSp       890 non-null int64\n",
      "Parch       890 non-null int64\n",
      "Fare        890 non-null float64\n",
      "Embarked    890 non-null object\n",
      "Title       890 non-null object\n",
      "dtypes: float64(2), int64(4), object(3)\n",
      "memory usage: 69.5+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle('trimmed_titanic_data.pkl')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By \"cleaned\" I mean I've derived titles (e.g. \"Mr.\", \"Mrs.\", \"Dr.\", etc) from the passenger names, imputed the missing Age values using polynomial regression with grid-searched 10-fold cross-validation, filled in the 3 missing Embarked values with the mode, and removed all fields that could be considered an id for that individual.\n",
    "\n",
    "Thus, there is no missing data.\n",
    "\n",
    "## Set categorical features as that type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 890 entries, 0 to 890\n",
      "Data columns (total 9 columns):\n",
      "Survived    890 non-null category\n",
      "Pclass      890 non-null category\n",
      "Sex         890 non-null category\n",
      "Age         890 non-null float64\n",
      "SibSp       890 non-null int64\n",
      "Parch       890 non-null int64\n",
      "Fare        890 non-null float64\n",
      "Embarked    890 non-null category\n",
      "Title       890 non-null category\n",
      "dtypes: category(5), float64(2), int64(2)\n",
      "memory usage: 39.3 KB\n"
     ]
    }
   ],
   "source": [
    "simulation_df = df.copy()\n",
    "\n",
    "categorical_features = ['Survived','Pclass','Sex','Embarked','Title']\n",
    "\n",
    "for feature in categorical_features:\n",
    "    simulation_df[feature] = simulation_df[feature].astype('category')\n",
    "    \n",
    "simulation_df.info()\n",
    "\n",
    "# df[\"A\"].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 890 entries, 0 to 890\n",
      "Data columns (total 17 columns):\n",
      "Age               890 non-null float64\n",
      "SibSp             890 non-null int64\n",
      "Parch             890 non-null int64\n",
      "Fare              890 non-null float64\n",
      "Survived_1        890 non-null uint8\n",
      "Pclass_2          890 non-null uint8\n",
      "Pclass_3          890 non-null uint8\n",
      "Sex_male          890 non-null uint8\n",
      "Embarked_Q        890 non-null uint8\n",
      "Embarked_S        890 non-null uint8\n",
      "Title_Dr          890 non-null uint8\n",
      "Title_Military    890 non-null uint8\n",
      "Title_Miss        890 non-null uint8\n",
      "Title_Mr          890 non-null uint8\n",
      "Title_Mrs         890 non-null uint8\n",
      "Title_Noble       890 non-null uint8\n",
      "Title_Rev         890 non-null uint8\n",
      "dtypes: float64(2), int64(2), uint8(13)\n",
      "memory usage: 46.1 KB\n"
     ]
    }
   ],
   "source": [
    "simulation_df = pd.get_dummies(simulation_df,drop_first=True)\n",
    "\n",
    "simulation_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 17 features.\n",
    "\n",
    "### Split into input/output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set output feature\n",
    "output_feature = 'Survived_1'\n",
    "\n",
    "# Get all column names\n",
    "column_names = list(simulation_df.columns)\n",
    "\n",
    "# Exclude one of every categorical variable since the other one-hot encodings cover everything\n",
    "input_features = [x for x in column_names if x != output_feature]\n",
    "\n",
    "# Split into features and responses\n",
    "X = simulation_df[input_features].copy()\n",
    "y = simulation_df[output_feature].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.61573034,  0.38426966])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulation_df['Survived_1'].value_counts().values/float(simulation_df['Survived_1'].value_counts().values.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, null accuracy of ~62% if always predict death.\n",
    "\n",
    "### Import data science library and initialize model collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cmshymansky/anaconda/lib/python2.7/site-packages/sklearn/lda.py:4: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n",
      "/Users/cmshymansky/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/cmshymansky/anaconda/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import data_science_lib as dsl\n",
    "\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic models w/ no pre-processing\n",
    "#### KNN\n",
    "Here I do a simple K-nearest neighbors (KNN) classification with 10-fold (default) cross-validation with a grid search over the default of 1 to 30 nearest neighbors and the use of either \"uniform\" or \"distance\" weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid parameters:\n",
      "estimator__n_neighbors : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "estimator__weights : ['uniform', 'distance']\n",
      "\n",
      "Training set classification accuracy:  0.72893258427\n",
      "\n",
      "Test set classification accuracy:  0.73595505618\n",
      "Confusion matrix: \n",
      "\n",
      " [[99  7]\n",
      " [40 32]]\n",
      "\n",
      "Normalized confusion matrix: \n",
      "\n",
      " [[ 0.55617978  0.03932584]\n",
      " [ 0.2247191   0.17977528]]\n",
      "\n",
      "Classification report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.93      0.81       106\n",
      "          1       0.82      0.44      0.58        72\n",
      "\n",
      "avg / total       0.76      0.74      0.71       178\n",
      "\n",
      "\n",
      "Best parameters:\n",
      "\n",
      "{'estimator__n_neighbors': 12, 'estimator__weights': 'uniform'}\n",
      "\n",
      " Pipeline(steps=[('estimator', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=12, p=2,\n",
      "           weights='uniform'))])\n",
      "CPU times: user 1.86 s, sys: 108 ms, total: 1.97 s\n",
      "Wall time: 2.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "reload(dsl)\n",
    "        \n",
    "# Figure out best model\n",
    "models['knn'] = dsl.train_model(X,y,\n",
    "                                use_default_param_dist=True,\n",
    "                                random_state=6,\n",
    "                                suppress_output=False, # Can suppress print outs if desired\n",
    "                                estimator='knn',) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out that the best settings are 30 neighbors and the use of the 'distance' weight.\n",
    "\n",
    "Note how I've set the random_state to 6 so that the models can be compared using the same test/train split.\n",
    "\n",
    "The output of the train_model() method is a Pipeline object with the optimal parameters found during the grid search and trained on all data.\n",
    "\n",
    "Additional fields have been added to the pipeline object. These extra parameters are the type of score ('regression' or 'classification'), '.score_type', the training score (L2 norm for regression and classification accuracy for classification), '.train_score', the corresponding test score, '.test_score'. \n",
    "\n",
    "For classification problems, additional parameters also include the confusion matrix, '.confusion_matrix', normalized confusion matrix, '.normalized_confusion_matrix', and the classification report, '.classification_report'.\n",
    "\n",
    "Here are the outputs of these additional parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: knn \n",
      "\n",
      "Training score:\t\t0.72893258427\n",
      "Test score:\t\t0.73595505618 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.93      0.81       106\n",
      "          1       0.82      0.44      0.58        72\n",
      "\n",
      "avg / total       0.76      0.74      0.71       178\n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "[[99  7]\n",
      " [40 32]] \n",
      "\n",
      "Normalized confusion matrix:\n",
      "\n",
      "[[ 0.55617978  0.03932584]\n",
      " [ 0.2247191   0.17977528]] \n",
      "\n",
      "Best parameters:\n",
      "{'estimator__n_neighbors': 12, 'estimator__weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "model_name = 'knn'\n",
    "\n",
    "pipeline = models[model_name]\n",
    "\n",
    "print 'Model:','knn','\\n'\n",
    "print 'Training score:\\t\\t', pipeline.train_score\n",
    "print 'Test score:\\t\\t', pipeline.test_score,'\\n'\n",
    "print pipeline.classification_report\n",
    "\n",
    "print 'Confusion matrix:\\n\\n',pipeline.confusion_matrix,'\\n'\n",
    "print 'Normalized confusion matrix:\\n\\n',pipeline.normalized_confusion_matrix,'\\n'\n",
    "print 'Best parameters:\\n', pipeline.best_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The print out from the solution above indicates that the default parameters to grid over are n_neighbors from 1 to 30 and the weights parameter as either 'uniform' or 'distance'.\n",
    "\n",
    "This can be changed in two different ways. One way is to overwrite the parameter values by setting the param_dist keyword argument with the use_default_param_dist set to True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "reload(dsl)\n",
    "\n",
    "# Set custom parameters\n",
    "param_dist = {\n",
    "    'estimator__n_neighbors': range(30,500)\n",
    "}\n",
    "\n",
    "# Figure out best model\n",
    "models['custom_overwrite_knn'] = dsl.train_model(X,y,\n",
    "                                       use_default_param_dist=True,\n",
    "                                       random_state=6,\n",
    "                                       suppress_output=False, # Can suppress print outs if desired\n",
    "                                       estimator='knn',\n",
    "                                       param_dist = param_dist) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to get a slightly better accuracy doing this.\n",
    "\n",
    "The second way to use different parameter grid values is to set them with the custom param_dist keyword argument yet set use_default_param_dist to False. This makes it so that you must set every single parameter manually.\n",
    "\n",
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "reload(dsl)\n",
    "\n",
    "# Set custom parameters\n",
    "param_dist = {\n",
    "    'estimator__n_neighbors': range(30,500)\n",
    "}\n",
    "\n",
    "# Figure out best model\n",
    "models['from_scratch_knn'] = dsl.train_model(X,y,\n",
    "                                       use_default_param_dist=False,\n",
    "                                       random_state=6,\n",
    "                                       suppress_output=False, # Can suppress print outs if desired\n",
    "                                       estimator='knn',\n",
    "                                       param_dist = param_dist) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the estimator__weights parameter isn't set for the KNN estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other models\n",
    "This code currently supports K-nearest neighbors, logistic regression, support vector machines, multilayer perceptrons, random forest, and adaboost. \n",
    "\n",
    "We can loop through and pick the best model like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "reload(dsl)\n",
    "\n",
    "# Set model names to iterate over\n",
    "model_names = ['knn','logistic_regression','svm','multilayer_perceptron','random_forest','adaboost']        \n",
    "\n",
    "# Cross-validate each model\n",
    "for model_name in model_names:\n",
    "    models[model_name] = dsl.train_model(X,y,\n",
    "                                    use_default_param_dist=True,\n",
    "                                    random_state=6,\n",
    "                                    estimator=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trained_model_names = models.keys()\n",
    "\n",
    "model_scores = [models[model].test_score for model in trained_model_names]\n",
    "\n",
    "max_score = max(model_scores)\n",
    "\n",
    "best_model = trained_model_names[model_scores.index(max_score)]\n",
    "\n",
    "print best_model,max_score, '\\n\\n',models[best_model].classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the best model is logistic regression with a classfication accuracy of ~88%.\n",
    "\n",
    "### Scaled data then classification\n",
    "We can specify the scale_type keyword argument to scale the data before being fed to the desired estimator. Currently only standard scaling is supported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "reload(dsl)\n",
    "\n",
    "# Set model names to iterate over\n",
    "model_names = ['knn','logistic_regression','svm','multilayer_perceptron','random_forest','adaboost']        \n",
    "\n",
    "# Cross-validate each model\n",
    "for model_name in model_names:\n",
    "    models['scaled_%s'%(model_name)] = dsl.train_model(X,y,\n",
    "                                                       use_default_param_dist=True,\n",
    "                                                       random_state=6,\n",
    "                                                       scale_type = 'standard',\n",
    "                                                       estimator=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trained_model_names = models.keys()\n",
    "\n",
    "model_scores = [models[model].test_score for model in trained_model_names]\n",
    "\n",
    "max_score = max(model_scores)\n",
    "\n",
    "best_model = trained_model_names[model_scores.index(max_score)]\n",
    "\n",
    "print best_model,max_score, '\\n\\n',models[best_model].classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection, scaling, and classification\n",
    "The feature_selection_type keyword argument can be used to select the best features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "reload(dsl)\n",
    "\n",
    "# Set model names to iterate over\n",
    "model_names = ['knn','logistic_regression','svm','multilayer_perceptron','random_forest','adaboost']        \n",
    "\n",
    "# Cross-validate each model\n",
    "for model_name in model_names:\n",
    "    models['select_scaled_%s'%(model_name)] = dsl.train_model(X,y,\n",
    "                                                       use_default_param_dist=True,\n",
    "                                                       random_state=6,\n",
    "                                                       scale_type = 'standard',\n",
    "                                                       feature_selection_type = 'select_k_best', \n",
    "                                                       estimator=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trained_model_names = models.keys()\n",
    "\n",
    "model_scores = [models[model].test_score for model in trained_model_names]\n",
    "\n",
    "max_score = max(model_scores)\n",
    "\n",
    "best_model = trained_model_names[model_scores.index(max_score)]\n",
    "\n",
    "print best_model,max_score, '\\n\\n',models[best_model].classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that logistic regression without scaling outperforms all combinations of scalling and the classifiers.\n",
    "\n",
    "### Scaled, transformed, then classification\n",
    "Setting the transform_type keyword argument allows the data to be transformed into a new coordinate system that is dependent on the algorithm.\n",
    "\n",
    "Currently, only principal component analysis (PCA) and t-distributed stochastic neighbor embedding (t-SNE) are supported.\n",
    "\n",
    "#### PCA transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "reload(dsl)\n",
    "\n",
    "# Set model names to iterate over\n",
    "model_names = ['knn','logistic_regression','svm','multilayer_perceptron','random_forest','adaboost']        \n",
    "\n",
    "# Cross-validate each model\n",
    "for model_name in model_names:\n",
    "    models['scaled_pca_%s'%(model_name)] = dsl.train_model(X,y,use_default_param_dist=True,\n",
    "                                                       random_state=6,\n",
    "                                                        transform_type='pca',\n",
    "                                                       scale_type = 'standard',\n",
    "                                                       estimator=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trained_model_names = models.keys()\n",
    "\n",
    "model_scores = [models[model].test_score for model in trained_model_names]\n",
    "\n",
    "max_score = max(model_scores)\n",
    "\n",
    "best_model = trained_model_names[model_scores.index(max_score)]\n",
    "\n",
    "print best_model,max_score,'\\n\\n',models[best_model].classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation with PCA doesn't appear to improve our results so far.\n",
    "\n",
    "#### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "reload(dsl)\n",
    "\n",
    "# Set model names to iterate over\n",
    "model_names = ['knn','logistic_regression','svm','multilayer_perceptron','random_forest','adaboost']        \n",
    "\n",
    "# Cross-validate each model\n",
    "for model_name in model_names:\n",
    "    models['scaled_t-sne_%s'%(model_name)] = dsl.train_model(X,y,use_default_param_dist=True,\n",
    "                                                       random_state=6,\n",
    "                                                        transform_type='t-sne',\n",
    "                                                       scale_type = 'standard',\n",
    "                                                       estimator=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trained_model_names = models.keys()\n",
    "\n",
    "model_scores = [models[model].test_score for model in trained_model_names]\n",
    "\n",
    "max_score = max(model_scores)\n",
    "\n",
    "best_model = trained_model_names[model_scores.index(max_score)]\n",
    "\n",
    "print best_model,max_score,'\\n\\n',models[best_model].classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression with with scaling and selection appears to outperform all other scenarios so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the best model's properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Best model:','logistic_regression','\\n'\n",
    "print 'Training score:\\t\\t', models['logistic_regression'].train_score\n",
    "print 'Test score:\\t\\t', models['logistic_regression'].test_score,'\\n'\n",
    "print models['logistic_regression'].classification_report\n",
    "\n",
    "print 'Confusion matrix:\\n\\n',models['logistic_regression'].confusion_matrix,'\\n'\n",
    "print 'Normalized confusion matrix:\\n\\n',models['logistic_regression'].normalized_confusion_matrix,'\\n'\n",
    "print 'Best parameters:\\n', models['logistic_regression'].best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir(models['logistic_regression'].steps[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.logspace(-10,10,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "reload(dsl)\n",
    "        \n",
    "param_dist = {\n",
    "    'estimator__C': np.logspace(-10,10,100),\n",
    "    'estimator__penalty': ['l2','l1']\n",
    "}\n",
    "    \n",
    "# Figure out best model\n",
    "models['custom_logistic_regression'] = dsl.train_model(X,y,\n",
    "                                use_default_param_dist=True,\n",
    "                                random_state=6,\n",
    "                                suppress_output=False, # Can suppress print outs if desired\n",
    "                                estimator='logistic_regression',\n",
    "                               param_dist=param_dist) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_coefficients = models['custom_logistic_regression'].steps[0][1].coef_[0]\n",
    "\n",
    "feature_ind = 0\n",
    "indices = []\n",
    "for feature in simulation_df.columns:\n",
    "    if feature != 'Survived_1':\n",
    "        print feature, '\\t',model_coefficients[feature_ind]\n",
    "        \n",
    "        feature_ind += 1\n",
    "        \n",
    "model_df = pd.Series(model_coefficients,index=[feature for feature in simulation_df.columns if feature != 'Survived_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_df.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like being a Reverend had a very negative effect on survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Survived'][df['Title']=='Dr'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were six reverends and all died."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next worst was having the title of \"Mr\". This is in contrast to just generally being a male."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[df['Title']=='Noble']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[(df['Sex']=='male')&(~df['Title'].isin(['Mr','Dr','Rev']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simulation_df[simulation_df['Title_Military']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print models['select_scaled_logistic_regression'].steps[2][1].coef_[0].shape\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_mask = models['select_scaled_logistic_regression'].steps[0][1].get_support()\n",
    "input_features = [feature for feature in simulation_df.columns if feature != 'Survived_1']\n",
    "\n",
    "model_coefficients = models['select_scaled_logistic_regression'].steps[2][1].coef_[0]\n",
    "\n",
    "feature_ind = 0\n",
    "for feature in np.array(input_features)[feature_mask]:\n",
    "    print feature, '\\t',model_coefficients[feature_ind]\n",
    "        \n",
    "    feature_ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in models['select_scaled_logistic_regression'].steps[2][1].coef_[0]:\n",
    "    print x\n",
    "\n",
    "print simulation_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_coefficients = models['select_scaled_logistic_regression'].steps[2][1].coef_[0]\n",
    "\n",
    "feature_ind = 0\n",
    "for feature in simulation_df.columns:\n",
    "    if feature != 'Survived_1': \n",
    "        print feature, '\\t',model_coefficients[feature_ind]\n",
    "        \n",
    "        feature_ind += 1\n",
    "        \n",
    "print model_coefficients[feature_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simulation_df[simulation_df['Title_Dr']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
