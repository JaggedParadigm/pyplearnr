{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyplearnr demo\n",
    "Here I demonstrate pyplearnr, a wrapper for building/training/validating scikit learn pipelines using GridSearchCV or RandomizedSearchCV.\n",
    "\n",
    "Quick keyword arguments give access to optional feature selection (e.g. SelectKBest), scaling (e.g. standard scaling), use of feature interactions, and data transformations (e.g. PCA, t-SNE) before being fed to a classifier/regressor.\n",
    "\n",
    "After building the pipeline, data can be used to perform a nested (stratified if classification) k-folds cross-validation and output an object containing data from the process, including the best model.\n",
    "\n",
    "Various default pipeline step parameters for the grid-search are available for quick iteration over different pipelines, with the option to ignore/override them in a flexible way.\n",
    "\n",
    "This is an on-going project that I intend to update with more models and pre-processing options and also with corresponding defaults.\n",
    "\n",
    "## Titanic dataset example\n",
    "Here I use the Titanic dataset I've cleaned and pickled in a separate tutorial.\n",
    "\n",
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 890 entries, 0 to 890\n",
      "Data columns (total 9 columns):\n",
      "Survived    890 non-null int64\n",
      "Pclass      890 non-null int64\n",
      "Sex         890 non-null object\n",
      "Age         890 non-null float64\n",
      "SibSp       890 non-null int64\n",
      "Parch       890 non-null int64\n",
      "Fare        890 non-null float64\n",
      "Embarked    890 non-null object\n",
      "Title       890 non-null object\n",
      "dtypes: float64(2), int64(4), object(3)\n",
      "memory usage: 69.5+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle('trimmed_titanic_data.pkl')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By \"cleaned\" I mean I've derived titles (e.g. \"Mr.\", \"Mrs.\", \"Dr.\", etc) from the passenger names, imputed the missing Age values using polynomial regression with grid-searched 10-fold cross-validation, filled in the 3 missing Embarked values with the mode, and removed all fields that could be considered an id for that individual.\n",
    "\n",
    "Thus, there is no missing/null data.\n",
    "\n",
    "## Set categorical features as type 'category'\n",
    "In order to one-hot encode categorical data, its best to set the features that are considered categorical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 890 entries, 0 to 890\n",
      "Data columns (total 9 columns):\n",
      "Survived    890 non-null category\n",
      "Pclass      890 non-null category\n",
      "Sex         890 non-null category\n",
      "Age         890 non-null float64\n",
      "SibSp       890 non-null int64\n",
      "Parch       890 non-null int64\n",
      "Fare        890 non-null float64\n",
      "Embarked    890 non-null category\n",
      "Title       890 non-null category\n",
      "dtypes: category(5), float64(2), int64(2)\n",
      "memory usage: 39.3 KB\n"
     ]
    }
   ],
   "source": [
    "simulation_df = df.copy()\n",
    "\n",
    "categorical_features = ['Survived','Pclass','Sex','Embarked','Title']\n",
    "\n",
    "for feature in categorical_features:\n",
    "    simulation_df[feature] = simulation_df[feature].astype('category')\n",
    "    \n",
    "simulation_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 890 entries, 0 to 890\n",
      "Data columns (total 17 columns):\n",
      "Age               890 non-null float64\n",
      "SibSp             890 non-null int64\n",
      "Parch             890 non-null int64\n",
      "Fare              890 non-null float64\n",
      "Survived_1        890 non-null uint8\n",
      "Pclass_2          890 non-null uint8\n",
      "Pclass_3          890 non-null uint8\n",
      "Sex_male          890 non-null uint8\n",
      "Embarked_Q        890 non-null uint8\n",
      "Embarked_S        890 non-null uint8\n",
      "Title_Dr          890 non-null uint8\n",
      "Title_Military    890 non-null uint8\n",
      "Title_Miss        890 non-null uint8\n",
      "Title_Mr          890 non-null uint8\n",
      "Title_Mrs         890 non-null uint8\n",
      "Title_Noble       890 non-null uint8\n",
      "Title_Rev         890 non-null uint8\n",
      "dtypes: float64(2), int64(2), uint8(13)\n",
      "memory usage: 46.1 KB\n"
     ]
    }
   ],
   "source": [
    "simulation_df = pd.get_dummies(simulation_df,drop_first=True)\n",
    "\n",
    "simulation_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 17 features.\n",
    "\n",
    "### Split into input/output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set output feature\n",
    "output_feature = 'Survived_1'\n",
    "\n",
    "# Get all column names\n",
    "column_names = list(simulation_df.columns)\n",
    "\n",
    "# Get input features\n",
    "input_features = [x for x in column_names if x != output_feature]\n",
    "\n",
    "# Split into features and responses\n",
    "X = simulation_df[input_features].copy()\n",
    "y = simulation_df[output_feature].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.61573034,  0.38426966])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulation_df['Survived_1'].value_counts().values/float(simulation_df['Survived_1'].value_counts().values.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, null accuracy of ~62% if always predict death.\n",
    "\n",
    "### Import pyplearnr and initialize optimized pipeline collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "import sys\n",
    "\n",
    "import os\n",
    "\n",
    "sys.path.append(\"./pyplearnr\")\n",
    "\n",
    "optimized_pipelines = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pipelines: 60 \n",
      "\n",
      "15 Pipeline(steps=(('estimator', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=8, p=2,\n",
      "           weights='distance')),))\n",
      "45 Pipeline(steps=(('estimator', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=23, p=2,\n",
      "           weights='distance')),))\n",
      "7 Pipeline(steps=(('estimator', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=4, p=2,\n",
      "           weights='distance')),))\n",
      "\n",
      "\n",
      "No model was chosen because there is no clear winner. Please use the same fit method with one of the indices above.\n",
      "\n",
      "Example:\tkfcv.fit(X.values, y.values, pipelines)\n",
      "\t\tkfcv.train_winning_pipeline(3)kfcv.fit(X.values, y.values, pipelines, best_outer_fold_pipeline=9)\n",
      "CPU times: user 5.08 s, sys: 121 ms, total: 5.2 s\n",
      "Wall time: 5.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "%autoreload\n",
    "\n",
    "import numpy as np\n",
    "import pyplearnr as ppl\n",
    "\n",
    "reload(ppl)\n",
    "\n",
    "kfcv = ppl.NestedKFoldCrossValidation()\n",
    "\n",
    "# kfcv = ppl.NestedKFoldCrossValidation(outer_loop_fold_count=3, \n",
    "#                                       inner_loop_fold_count=3, \n",
    "#                                       shuffle_flag=True,\n",
    "#                                       shuffle_seed=3736,\n",
    "#                                       outer_loop_split_seed=503,\n",
    "#                                       inner_loop_split_seeds=[443, 286, 203])\n",
    "\n",
    "pipeline_bundle_schematic = [\n",
    "#     {'scaler': {\n",
    "#         'standard': {}\n",
    "#     }},\n",
    "    {'estimator': {\n",
    "        'knn': {\n",
    "            'n_neighbors': range(1,31),\n",
    "            'weights': ['uniform', 'distance']\n",
    "        }\n",
    "#         'svm': {\n",
    "#             'C': np.array([1.00000000e+00])\n",
    "#         }\n",
    "    }}\n",
    "]\n",
    "\n",
    "pipelines = ppl.PipelineBuilder().build_pipeline_bundle(pipeline_bundle_schematic)\n",
    "print 'Number of pipelines: %d'%(len(pipelines)), '\\n'\n",
    "\n",
    "kfcv.fit(X.values, y.values, pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Pipeline(steps=(('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('estimator', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=24, p=2,\n",
      "           weights='distance'))))\n",
      "4 Pipeline(steps=(('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('estimator', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=22, p=2,\n",
      "           weights='uniform'))))\n",
      "5 Pipeline(steps=(('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('estimator', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=22, p=2,\n",
      "           weights='distance'))))\n",
      "\n",
      "\n",
      "No model was chosen because there is no clear winner. Please use the same fit method with one of the indices above.\n",
      "\n",
      "Example:\tkfcv.fit(X.values, y.values, pipelines)\n",
      "\t\tkfcv.train_winning_pipeline(3)kfcv.fit(X.values, y.values, pipelines, best_outer_fold_pipeline=9)\n"
     ]
    }
   ],
   "source": [
    "best_inner_fold_pipelines = {\n",
    "    2: 9\n",
    "}\n",
    "\n",
    "kfcv.fit(X.values, y.values, pipelines, \n",
    "         best_inner_fold_pipeline_inds = best_inner_fold_pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "Best pipeline:\t\t45\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "Validation performance\n",
      "----------------------\n",
      "0\t1\t2\tmean\tstd\t0%\t25%\t50%\t75%\t100%\n",
      "0.70652\t0.66752\t0.72438\t0.69947\t0.02908\t0.66752\t0.68702\t0.70652\t0.71545\t0.72438\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "Pipeline steps\n",
      "---------------\n",
      "estimator:\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "   metric_params=None, n_jobs=1, n_neighbors=23, p=2,\n",
      "   weights='distance')\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "Inter-loop performance\n",
      "-----------------------\n",
      "\t0\t1\t2\tmean\tstd\t0%\t25%\t50%\t75%\t100%\n",
      "0\t0.66632\t0.7137\t0.67455\t0.68485\t0.02532\t0.66632\t0.67043\t0.67455\t0.69412\t0.7137\n",
      "1\t0.66943\t0.68807\t0.71859\t0.69203\t0.02482\t0.66943\t0.67875\t0.68807\t0.70333\t0.71859\n",
      "2\t0.72163\t0.71429\t0.67672\t0.70421\t0.02409\t0.67672\t0.6955\t0.71429\t0.71796\t0.72163\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "Nested k-fold cross-validation parameters\n",
      "-----------------------------------------\n",
      "scoring metric:\t\t\tauc\n",
      "\n",
      "scoring type:\t\t\tmedian\n",
      "\n",
      "outer-fold count:\t\t3\n",
      "inner-fold count:\t\t3\n",
      "\n",
      "shuffle seed:\t\t\t3556\n",
      "outer-loop split seed:\t\t560\n",
      "inner-loop split seeds:\t\t237, 536, 386\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_outer_fold_pipeline = 45\n",
    "\n",
    "kfcv.fit(X.values, y.values, pipelines, \n",
    "         best_outer_fold_pipeline = best_outer_fold_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interactions_knn\n",
      "interactions_logistic_regression\n",
      "interactions_svm\n",
      "interactions_multilayer_perceptron\n",
      "interactions_random_forest\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-d12264033cd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu\"%autoreload\\nimport itertools\\n\\nestimators = ['knn','logistic_regression','svm',\\n              'multilayer_perceptron','random_forest','adaboost']\\n\\nfeature_interaction_options = [True,False]\\nfeature_selection_options = [None,'select_k_best']\\nscaling_options = [None,'standard','normal','min_max','binary']\\ntransformations = [None,'pca']\\n\\npipeline_steps = [feature_interaction_options,feature_selection_options,scaling_options,\\n                  transformations,estimators]\\n\\npipeline_options = list(itertools.product(*pipeline_steps))\\n\\noptimized_pipelines = {}\\n\\nfor pipeline_step_combo in pipeline_options:\\n    model_name = []\\n    \\n    feature_interactions = pipeline_step_combo[0]\\n    \\n    if feature_interactions:\\n        model_name.append('interactions')\\n        \\n    feature_selection_type = pipeline_step_combo[1]\\n    \\n    if feature_selection_type:\\n        model_name.append('select')\\n    \\n    scale_type = pipeline_step_combo[2]\\n    \\n    if scale_type:\\n        model_name.append(scale_type)\\n        \\n    transform_type = pipeline_step_combo[3]\\n    \\n    if transform_type:\\n        model_name.append(transform_type)\\n        \\n    estimator = pipeline_step_combo[4]\\n    \\n    model_name.append(estimator)\\n    \\n    model_name = '_'.join(model_name)\\n    \\n    print model_name\\n    \\n    # Set pipeline keyword arguments\\n    optimized_pipeline_kwargs = {\\n        'feature_selection_type': feature_selection_type,\\n        'scale_type': scale_type,\\n        'transform_type': transform_type\\n        }\\n\\n    # Initialize pipeline\\n    optimized_pipeline = ppl.PipelineOptimization(estimator,**optimized_pipeline_kwargs)\\n\\n    # Set pipeline fitting parameters\\n    fit_kwargs = {\\n        'cv': 10,\\n        'num_parameter_combos': None,\\n        'n_jobs': -1,\\n        'random_state': None,\\n        'suppress_output': True,\\n        'use_default_param_dist': True,\\n        'param_dist': None,\\n        'test_size': 0.2 # 20% saved as test set\\n    }\\n\\n    # Fit data\\n    optimized_pipeline.fit(X,y,**fit_kwargs)\\n\\n    # Save optimized pipeline\\n    optimized_pipelines[model_name] = optimized_pipeline\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/cmshymansky/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/cmshymansky/anaconda/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cmshymansky/anaconda/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/cmshymansky/Documents/code/machine_learning/tools/pyplearnr/pyplearnr/old.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cv, num_parameter_combos, n_jobs, random_state, suppress_output, use_default_param_dist, param_dist, test_size)\u001b[0m\n\u001b[1;32m    314\u001b[0m                                                   \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                                                   \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m                                                   num_parameter_combos=num_parameter_combos)\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;31m# Save pipeline metrics and predicated values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cmshymansky/Documents/code/machine_learning/tools/pyplearnr/pyplearnr/old.py\u001b[0m in \u001b[0;36moptimize_pipeline\u001b[0;34m(self, X_train, y_train, pipeline, scoring, param_dist, cv, n_jobs, num_parameter_combos)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# Perform grid search using above parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m         \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0;31m# Return grid_search object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cmshymansky/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtest\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \"\"\"\n\u001b[0;32m--> 940\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cmshymansky/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, groups, parameter_iterable)\u001b[0m\n\u001b[1;32m    560\u001b[0m                                   \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m--> 562\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m           for train, test in cv.split(X, y, groups))\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cmshymansky/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cmshymansky/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%autoreload\n",
    "import itertools\n",
    "\n",
    "estimators = ['knn','logistic_regression','svm',\n",
    "              'multilayer_perceptron','random_forest','adaboost']\n",
    "\n",
    "feature_interaction_options = [True,False]\n",
    "feature_selection_options = [None,'select_k_best']\n",
    "scaling_options = [None,'standard','normal','min_max','binary']\n",
    "transformations = [None,'pca']\n",
    "\n",
    "pipeline_steps = [feature_interaction_options,feature_selection_options,scaling_options,\n",
    "                  transformations,estimators]\n",
    "\n",
    "pipeline_options = list(itertools.product(*pipeline_steps))\n",
    "\n",
    "optimized_pipelines = {}\n",
    "\n",
    "for pipeline_step_combo in pipeline_options:\n",
    "    model_name = []\n",
    "    \n",
    "    feature_interactions = pipeline_step_combo[0]\n",
    "    \n",
    "    if feature_interactions:\n",
    "        model_name.append('interactions')\n",
    "        \n",
    "    feature_selection_type = pipeline_step_combo[1]\n",
    "    \n",
    "    if feature_selection_type:\n",
    "        model_name.append('select')\n",
    "    \n",
    "    scale_type = pipeline_step_combo[2]\n",
    "    \n",
    "    if scale_type:\n",
    "        model_name.append(scale_type)\n",
    "        \n",
    "    transform_type = pipeline_step_combo[3]\n",
    "    \n",
    "    if transform_type:\n",
    "        model_name.append(transform_type)\n",
    "        \n",
    "    estimator = pipeline_step_combo[4]\n",
    "    \n",
    "    model_name.append(estimator)\n",
    "    \n",
    "    model_name = '_'.join(model_name)\n",
    "    \n",
    "    print model_name\n",
    "    \n",
    "    # Set pipeline keyword arguments\n",
    "    optimized_pipeline_kwargs = {\n",
    "        'feature_selection_type': feature_selection_type,\n",
    "        'scale_type': scale_type,\n",
    "        'transform_type': transform_type\n",
    "        }\n",
    "\n",
    "    # Initialize pipeline\n",
    "    optimized_pipeline = ppl.PipelineOptimization(estimator,**optimized_pipeline_kwargs)\n",
    "\n",
    "    # Set pipeline fitting parameters\n",
    "    fit_kwargs = {\n",
    "        'cv': 10,\n",
    "        'num_parameter_combos': None,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': None,\n",
    "        'suppress_output': True,\n",
    "        'use_default_param_dist': True,\n",
    "        'param_dist': None,\n",
    "        'test_size': 0.2 # 20% saved as test set\n",
    "    }\n",
    "\n",
    "    # Fit data\n",
    "    optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "\n",
    "    # Save optimized pipeline\n",
    "    optimized_pipelines[model_name] = optimized_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN with and without pre-processing and various options\n",
    "#### Basic KNN\n",
    "Here we do a K-nearest neighbors (KNN) classification with stratified 10-fold (default) cross-validation with a grid search over the default of 1 to 30 nearest neighbors and the use of either \"uniform\" or \"distance\" weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.23 s, sys: 112 ms, total: 2.34 s\n",
      "Wall time: 5.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "estimator = 'knn'\n",
    "\n",
    "# Set pipeline keyword arguments\n",
    "optimized_pipeline_kwargs = {\n",
    "    'feature_selection_type': None,\n",
    "    'scale_type': None,\n",
    "    'transform_type': None\n",
    "    }\n",
    "\n",
    "# Initialize pipeline\n",
    "optimized_pipeline = ppl.PipelineOptimization(estimator,**optimized_pipeline_kwargs)\n",
    "\n",
    "# Set pipeline fitting parameters\n",
    "fit_kwargs = {\n",
    "    'cv': 10,\n",
    "    'num_parameter_combos': None,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 6,\n",
    "    'suppress_output': True,\n",
    "    'use_default_param_dist': True,\n",
    "    'param_dist': None,\n",
    "    'test_size': 0.2 # 20% saved as test set\n",
    "}\n",
    "\n",
    "# Fit data\n",
    "optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "\n",
    "# Save \n",
    "optimized_pipelines[estimator] = optimized_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the default OptimizedPipeline parameters and those for its fit() method.\n",
    "\n",
    "The OptimizedPipeline class contains all of the data associated with the nested stratified k-folds cross-validation.\n",
    "\n",
    "After use of the fit() method, this includes the data, its test/train splits (based on the test_size percentage keyword argument), the GridSearchCV or RandomizedGridSearchCV object, the Pipeline object that has been retrained using all of the data with the best parameters, test/train scores, and validation metrics/reports.\n",
    "\n",
    "A report can be printed immediately after the fit by setting the suppress_output keyword argument to True. \n",
    "\n",
    "Printing the OptimizedPipeline instance also shows the report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline:\n",
      "\n",
      "1: estimator        KNeighborsClassifier           \n",
      "                                                   \n",
      "                    n_neighbors          = 11        \n",
      "                    n_jobs               = 1         \n",
      "                    algorithm            = auto      \n",
      "                    metric               = minkowski \n",
      "                    metric_params        = None      \n",
      "                    p                    = 2         \n",
      "                    weights              = distance  \n",
      "                    leaf_size            = 30        \n",
      "                                                   \n",
      "\n",
      "Training set classification accuracy:\t0.730\n",
      "Test set classification accuracy:\t0.770\n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "[[93 17]\n",
      " [24 44]]\n",
      "\n",
      "Normalized confusion matrix:\n",
      "\n",
      "[[ 0.52247191  0.09550562]\n",
      " [ 0.13483146  0.24719101]]\n",
      "\n",
      "Classification report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.85      0.82       110\n",
      "          1       0.72      0.65      0.68        68\n",
      "\n",
      "avg / total       0.77      0.77      0.77       178\n",
      "\n",
      "\n",
      "Grid search parameters:\n",
      "\n",
      "                    scoring              = accuracy  \n",
      "                    n_jobs               = -1        \n",
      "                    verbose              = 0         \n",
      "                    fit_params           = {}        \n",
      "                    refit                = 1         \n",
      "                    return_train_score   = 1         \n",
      "                    iid                  = 1         \n",
      "                    error_score          = raise     \n",
      "                    pre_dispatch         = 2*n_jobs  \n",
      "                    param_grid           = {'estimator__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30], 'estimator__weights': ['uniform', 'distance']}\n",
      "                    cv                   = 10        \n",
      "                                                   \n",
      "Best parameters:                                   \n",
      "                                                   \n",
      "                    estimator__n_neighbors = 11        \n",
      "                    estimator__weights   = distance  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print optimized_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The report lists the steps in the pipeline, their optimized settings, the test/training accuracy (or L2 regression score), the grid search parameters, and the best parameters.\n",
    "\n",
    "If the estimator used is a classifier it also includes the confusion matrix, normalized confusion matrix, and a classification report containing precision/recall/f1-score for each class.\n",
    "\n",
    "Turns out that the best settings for this optimized pipeline are 12 neighbors and the use of the 'uniform' weight.\n",
    "\n",
    "Note how I've set the random_state keyword agument to 6 so that the models can be compared using the same test/train split.\n",
    "\n",
    "#### Default pipeline step grid parameters\n",
    "The default parameters to grid-search over for k-nearest neighbors are 1 to 30 neighbors and either the 'uniform' or 'distance' weight.\n",
    "\n",
    "The defaults for the pre-processing steps, classifiers, and regressors can be viewed by using the get_default_pipeline_step_parameters() method with the number of features as the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pre_processing_grid_parameters,classifier_grid_parameters,regression_grid_parameters = \\\n",
    "optimized_pipeline.get_default_pipeline_step_parameters(X.shape[0])\n",
    "\n",
    "classifier_grid_parameters['knn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN with custom pipeline step grid parameters\n",
    "These default parameters can be ignored by setting the use_default_param_dist keyword argument to False. \n",
    "\n",
    "The param_dist keyword argument can be used to keep default parameters (if use_default_param_dist set to True) or to be used as the sole source of parameters (if use_default_param_dist set to False). \n",
    "\n",
    "Here is a demonstration of generation of default parameters with those in param_dist being overridden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "estimator_name = 'knn'\n",
    "\n",
    "model_name = 'custom_override_%s'%(estimator_name)\n",
    "\n",
    "# Set custom parameters\n",
    "param_dist = {\n",
    "    'estimator__n_neighbors': range(30,500)\n",
    "}\n",
    "\n",
    "# Initialize pipeline\n",
    "optimized_pipeline = ppl.OptimizedPipeline(estimator)\n",
    "\n",
    "# Set pipeline fitting parameters\n",
    "fit_kwargs = {\n",
    "    'random_state': 6,\n",
    "    'param_dist': param_dist,\n",
    "}\n",
    "\n",
    "# Fit data\n",
    "optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "\n",
    "# Save \n",
    "optimized_pipelines[model_name] = optimized_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the n_neighbors parameter was 30 to 499 instead of 1 to 30.\n",
    "\n",
    "Here's an example of only using param_dist for parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_name = 'from_scratch_%s'%(estimator_name)\n",
    "\n",
    "# Set custom parameters\n",
    "param_dist = {\n",
    "    'estimator__n_neighbors': range(10,30)\n",
    "}\n",
    "\n",
    "estimator = 'knn'\n",
    "\n",
    "# Initialize pipeline\n",
    "optimized_pipeline = ppl.OptimizedPipeline(estimator)\n",
    "\n",
    "# Set pipeline fitting parameters\n",
    "fit_kwargs = {\n",
    "    'random_state': 6,\n",
    "    'use_default_param_dist': False,\n",
    "    'param_dist': param_dist,\n",
    "}\n",
    "\n",
    "# Fit data\n",
    "optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "\n",
    "# Save \n",
    "optimized_pipelines[model_name] = optimized_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the estimator\\_\\_weights parameter isn't set for the KNN estimator.\n",
    "\n",
    "### KNN with scaling\n",
    "The currently supported scaling options are standard, normal, min-max, and binary using scikit-learn's StandardScaler, Normalizer, MinMaxScaler, and Binarizer, respectively. These are set by the pipeline initialization kwarg 'scale_type' like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "estimator = 'knn'\n",
    "\n",
    "scaling_options = ['standard','normal','min-max','binary']\n",
    "\n",
    "for scaling_option in scaling_options:\n",
    "    model_name = '%s_%s'%(scaling_option,estimator_name)\n",
    "    \n",
    "    optimized_pipeline_kwargs = {\n",
    "        'scale_type': scaling_option\n",
    "    }\n",
    "\n",
    "    # Initialize pipeline\n",
    "    optimized_pipeline = ppl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "\n",
    "    # Set pipeline fitting parameters\n",
    "    fit_kwargs = {\n",
    "        'random_state': 6,\n",
    "        'use_default_param_dist': True,\n",
    "        'suppress_output': True\n",
    "        \n",
    "    }\n",
    "\n",
    "    # Fit data\n",
    "    optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "\n",
    "    # Save \n",
    "    optimized_pipelines[model_name] = optimized_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the pipelines so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline_keys = optimized_pipelines.keys()\n",
    "\n",
    "test_scores = [optimized_pipelines[key].test_score_ for key in pipeline_keys]\n",
    "\n",
    "ax = pd.Series(test_scores,index=pipeline_keys).sort_values().plot(kind='barh',color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary scaling fed into a KNN classifier appears to have the best training score.\n",
    "\n",
    "#### KNN with custom min-max and binary scaling settings\n",
    "MinMaxScaler scales each feature value to between 0 and 1 by default. Different scaling ranges can be gridded over by setting the 'scaler\\_\\_feature_range' keyword argument in param_dist.\n",
    "\n",
    "Binarizer sets each value to 0 or 1 depending on a threshold. The default for pyplearnr is 0.5. This can be changed by setting 'scaler\\_\\_threshold' using param_dist.\n",
    "\n",
    "Here is an example of setting both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reload(ppl)\n",
    "\n",
    "estimator = 'knn'\n",
    "\n",
    "scaling_options = ['min_max','binary']\n",
    "\n",
    "param_dists = {\n",
    "    'min_max': {\n",
    "        'scaler__feature_range': [(1,2),(3,4)]\n",
    "    },\n",
    "    'binary': {\n",
    "        'scaler__threshold': np.arange(0,1,0.1)\n",
    "    }\n",
    "}\n",
    "\n",
    "for scaling_option in scaling_options:\n",
    "    model_name = 'custom_%s_%s'%(scaling_option,estimator_name)\n",
    "    \n",
    "    optimized_pipeline_kwargs = {\n",
    "        'scale_type': scaling_option\n",
    "    }\n",
    "\n",
    "    # Initialize pipeline\n",
    "    optimized_pipeline = ppl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "    \n",
    "    # Set pipeline fitting parameters\n",
    "    fit_kwargs = {\n",
    "        'random_state': 6,\n",
    "        'use_default_param_dist': True,\n",
    "        'suppress_output': True,\n",
    "        'param_dist': param_dists[scaling_option]\n",
    "        \n",
    "    }\n",
    "\n",
    "    # Fit data\n",
    "    optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "\n",
    "    # Save \n",
    "    optimized_pipelines[model_name] = optimized_pipeline\n",
    "    \n",
    "# Visualize results\n",
    "pipeline_keys = optimized_pipelines.keys()\n",
    "\n",
    "test_scores = [optimized_pipelines[key].test_score_ for key in pipeline_keys]\n",
    "\n",
    "ax = pd.Series(test_scores,index=pipeline_keys).sort_values().plot(kind='barh',color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switching the range for min_max scaling boosted it to rank 1 for pipeline training scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print optimized_pipelines['custom_min_max_knn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of 1 to 2 for the MinMaxScaler appeared to be the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN with feature selection using SelectKBest with f_classif\n",
    "Currently only one form of feature selection, SelectKBest with f_classif, is supported. This is set using the 'feature_selection_type' keyword argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reload(ppl)\n",
    "\n",
    "estimator = 'knn'\n",
    "\n",
    "model_name = 'select_%s'%(estimator_name)\n",
    "\n",
    "optimized_pipeline_kwargs = {\n",
    "    'feature_selection_type': 'select_k_best'\n",
    "}\n",
    "\n",
    "# Initialize pipeline\n",
    "optimized_pipeline = ppl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "\n",
    "# Set pipeline fitting parameters\n",
    "fit_kwargs = {\n",
    "    'random_state': 6,\n",
    "    'use_default_param_dist': True,\n",
    "    'suppress_output': True\n",
    "}\n",
    "\n",
    "# Fit data\n",
    "optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "\n",
    "# Save \n",
    "optimized_pipelines[model_name] = optimized_pipeline\n",
    "    \n",
    "# Visualize results\n",
    "pipeline_keys = optimized_pipelines.keys()\n",
    "\n",
    "test_scores = [optimized_pipelines[key].test_score_ for key in pipeline_keys]\n",
    "\n",
    "ax = pd.Series(test_scores,index=pipeline_keys).sort_values().plot(kind='barh',color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection and KNN did had a mid-level training score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print optimized_pipelines['select_knn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SelectKBest with f_classif chose 5 features as the best to use in the model.\n",
    "\n",
    "The features selected by SelectKBest can be accessed normally, using the mask obtained from the get_support() method on the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_selection_mask = optimized_pipelines['select_knn'].pipeline.named_steps['feature_selection'].get_support()\n",
    "\n",
    "print np.array(X.columns)[feature_selection_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, Pclass 3, being male, and the titles Miss, Mr, and Mrs were considered the most important features by SelectKBest using f_classif.\n",
    "\n",
    "#### Setting custom feature selection\n",
    "The default number of features is 1 to all of them. This can be gridded over different values by setting 'feature_selection\\_\\_k' in param_dist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reload(ppl)\n",
    "\n",
    "estimator = 'knn'\n",
    "\n",
    "model_name = 'custom_select_%s'%(estimator_name)\n",
    "\n",
    "optimized_pipeline_kwargs = {\n",
    "    'feature_selection_type': 'select_k_best'\n",
    "}\n",
    "\n",
    "# Initialize pipeline\n",
    "optimized_pipeline = ppl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "\n",
    "param_dist = {\n",
    "    'feature_selection__k': [5,7,8]\n",
    "}\n",
    "\n",
    "# Set pipeline fitting parameters\n",
    "fit_kwargs = {\n",
    "    'random_state': 6,\n",
    "    'use_default_param_dist': True,\n",
    "    'suppress_output': True,\n",
    "    'param_dist': param_dist\n",
    "}\n",
    "\n",
    "# Fit data\n",
    "optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "\n",
    "# Save \n",
    "optimized_pipelines[model_name] = optimized_pipeline\n",
    "    \n",
    "# Visualize results\n",
    "pipeline_keys = optimized_pipelines.keys()\n",
    "\n",
    "test_scores = [optimized_pipelines[key].test_score_ for key in pipeline_keys]\n",
    "\n",
    "ax = pd.Series(test_scores,index=pipeline_keys).sort_values().plot(kind='barh',color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print optimized_pipelines['custom_select_knn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN using feature interactions\n",
    "Feature products of different degrees can be used as additional features by setting the 'feature_interaction' OptimizedPipeline keyword argument to True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reload(ppl)\n",
    "\n",
    "estimator = 'knn'\n",
    "\n",
    "model_name = 'interaction_%s'%(estimator_name)\n",
    "\n",
    "optimized_pipeline_kwargs = {\n",
    "    'feature_interactions': True\n",
    "}\n",
    "\n",
    "# Initialize pipeline\n",
    "optimized_pipeline = ppl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "\n",
    "# Set pipeline fitting parameters\n",
    "fit_kwargs = {\n",
    "    'random_state': 6,\n",
    "    'use_default_param_dist': True,\n",
    "    'suppress_output': True\n",
    "}\n",
    "\n",
    "# Fit data\n",
    "optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "\n",
    "# Save \n",
    "optimized_pipelines[model_name] = optimized_pipeline\n",
    "    \n",
    "# Visualize results\n",
    "pipeline_keys = optimized_pipelines.keys()\n",
    "\n",
    "test_scores = [optimized_pipelines[key].test_score_ for key in pipeline_keys]\n",
    "\n",
    "ax = pd.Series(test_scores,index=pipeline_keys).sort_values().plot(kind='barh',color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print optimized_pipelines['interaction_knn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal number of interactions (number of features multiplied by each other at once) was found to be 1.\n",
    "\n",
    "#### KNN using custom number of feature interactions\n",
    "The 'feature_interactions__degree' dictates the number of interactions. The default setting is to try no interactions (degree 1) and 2 interactions. Setting this in param_dist allows custom numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reload(ppl)\n",
    "\n",
    "estimator = 'knn'\n",
    "\n",
    "model_name = 'custom_interaction_%s'%(estimator_name)\n",
    "\n",
    "optimized_pipeline_kwargs = {\n",
    "    'feature_interactions': True\n",
    "}\n",
    "\n",
    "# Initialize pipeline\n",
    "optimized_pipeline = ppl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "\n",
    "param_dist = {\n",
    "    'feature_interactions__degree': [2,3,4]\n",
    "}\n",
    "\n",
    "# Set pipeline fitting parameters\n",
    "fit_kwargs = {\n",
    "    'random_state': 6,\n",
    "    'use_default_param_dist': True,\n",
    "    'suppress_output': True,\n",
    "    'param_dist': param_dist\n",
    "}\n",
    "\n",
    "# Fit data\n",
    "optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "\n",
    "# Save \n",
    "optimized_pipelines[model_name] = optimized_pipeline\n",
    "    \n",
    "# Visualize results\n",
    "pipeline_keys = optimized_pipelines.keys()\n",
    "\n",
    "test_scores = [optimized_pipelines[key].test_score_ for key in pipeline_keys]\n",
    "\n",
    "ax = pd.Series(test_scores,index=pipeline_keys).sort_values().plot(kind='barh',color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print optimized_pipelines['custom_interaction_knn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN with pre-processing transforms\n",
    "Currently Principal Component Analysis (PCA) and t-distributed stochastic neighbor embedding (t-SNE) are supported as pre-processing options.\n",
    "\n",
    "#### KNN with PCA pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reload(ppl)\n",
    "\n",
    "estimator = 'knn'\n",
    "\n",
    "model_name = 'pca_%s'%(estimator_name)\n",
    "\n",
    "optimized_pipeline_kwargs = {\n",
    "    'transform_type': 'pca'\n",
    "}\n",
    "\n",
    "# Initialize pipeline\n",
    "optimized_pipeline = ppl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "\n",
    "# Set pipeline fitting parameters\n",
    "fit_kwargs = {\n",
    "    'random_state': 6,\n",
    "    'use_default_param_dist': True,\n",
    "    'suppress_output': True\n",
    "}\n",
    "\n",
    "# Fit data\n",
    "optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "\n",
    "# Save \n",
    "optimized_pipelines[model_name] = optimized_pipeline\n",
    "    \n",
    "# Visualize results\n",
    "pipeline_keys = optimized_pipelines.keys()\n",
    "\n",
    "test_scores = [optimized_pipelines[key].test_score_ for key in pipeline_keys]\n",
    "\n",
    "ax = pd.Series(test_scores,index=pipeline_keys).sort_values().plot(kind='barh',color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print optimized_pipelines['pca_knn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the transformed data after PCA normally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformed_data = optimized_pipelines['pca_knn'].pipeline.named_steps['transform'].transform(X.values)\n",
    "\n",
    "column_names = ['PCA_%d'%(feature_ind+1) for feature_ind in range(transformed_data.shape[1])]\n",
    "\n",
    "pca_df = pd.DataFrame(transformed_data,columns=column_names)\n",
    "\n",
    "pca_df.plot(x='PCA_1',y='PCA_2',style='ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is currently a very manual process and would be difficult with more and more processing steps. I'm thinking of automating this with a class containing all optimized pipelines in the future.\n",
    "\n",
    "Any of the parameters displayed in the pipeline section of the report (iterated_power, random_state, whiten, n_components, etc) can be set in param_dist by 'transform\\__setting' as done previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN with t-SNE pre-processing\n",
    "The t-SNE algorithm can be used as a pre-processing algorithm as well by setting the 'transform_type' keyword argument to 't-sne':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reload(ppl)\n",
    "\n",
    "estimator = 'knn'\n",
    "\n",
    "model_name = 't-sne_%s'%(estimator_name)\n",
    "\n",
    "optimized_pipeline_kwargs = {\n",
    "    'transform_type': 't-sne'\n",
    "}\n",
    "\n",
    "# Initialize pipeline\n",
    "optimized_pipeline = ppl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "\n",
    "# Set pipeline fitting parameters\n",
    "fit_kwargs = {\n",
    "    'random_state': 6,\n",
    "    'use_default_param_dist': True,\n",
    "    'suppress_output': True\n",
    "}\n",
    "\n",
    "# Fit data\n",
    "optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "\n",
    "# Save \n",
    "optimized_pipelines[model_name] = optimized_pipeline\n",
    "    \n",
    "# Visualize results\n",
    "pipeline_keys = optimized_pipelines.keys()\n",
    "\n",
    "test_scores = [optimized_pipelines[key].test_score_ for key in pipeline_keys]\n",
    "\n",
    "ax = pd.Series(test_scores,index=pipeline_keys).sort_values().plot(kind='barh',color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This t-SNE step takes longer than most in pyplearnr unfortunately. It also resulted in the worst score. I'll try to optimize this in the future.\n",
    "\n",
    "### Reducing the number of grid combinations\n",
    "Setting the 'num_parameter_combos' fit() method keyword argument to an integer will limit the number of grid combinations to perform using RandomizedSearchCV instead of GridSearchCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reload(ppl)\n",
    "\n",
    "estimator = 'knn'\n",
    "\n",
    "model_name = 'less_combos_%s'%(estimator_name)\n",
    "\n",
    "optimized_pipeline_kwargs = {}\n",
    "\n",
    "# Initialize pipeline\n",
    "optimized_pipeline = ppl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "\n",
    "# Set pipeline fitting parameters\n",
    "fit_kwargs = {\n",
    "    'random_state': 6,\n",
    "    'use_default_param_dist': True,\n",
    "    'suppress_output': True,\n",
    "    'num_parameter_combos': 5\n",
    "}\n",
    "\n",
    "# Fit data\n",
    "optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "\n",
    "# Save \n",
    "optimized_pipelines[model_name] = optimized_pipeline\n",
    "    \n",
    "# Visualize results\n",
    "pipeline_keys = optimized_pipelines.keys()\n",
    "\n",
    "test_scores = [optimized_pipelines[key].test_score_ for key in pipeline_keys]\n",
    "\n",
    "ax = pd.Series(test_scores,index=pipeline_keys).sort_values().plot(kind='barh',color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good way to speed up computations and give you an idea as to how long a particular pipeline takes to train. \n",
    "\n",
    "Here's the corresponding report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print optimized_pipelines['less_combos_knn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameter combination, of those attempted by RandomizedSearchCV, was 12 nearest neighbors with the 'uniform' weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other models\n",
    "\n",
    "This code currently supports K-nearest neighbors, logistic regression, support vector machines, multilayer perceptrons, random forest, and adaboost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "classifiers = ['knn','logistic_regression','svm',\n",
    "               'multilayer_perceptron','random_forest','adaboost']\n",
    "\n",
    "for estimator in classifiers:\n",
    "    # Set pipeline keyword arguments\n",
    "    optimized_pipeline_kwargs = {}\n",
    "\n",
    "    # Initialize pipeline\n",
    "    optimized_pipeline = ppl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "    \n",
    "    # Set pipeline fitting parameters\n",
    "    fit_kwargs = {\n",
    "        'random_state': 6,\n",
    "        'suppress_output': True,\n",
    "        'use_default_param_dist': True\n",
    "    }\n",
    "    \n",
    "    # Fit data\n",
    "    optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "    \n",
    "    # Save \n",
    "    optimized_pipelines[estimator] = optimized_pipeline\n",
    "    \n",
    "# Visualize results\n",
    "pipeline_keys = optimized_pipelines.keys()\n",
    "\n",
    "test_scores = [optimized_pipelines[key].test_score_ for key in pipeline_keys]\n",
    "\n",
    "ax = pd.Series(test_scores,index=pipeline_keys).sort_values().plot(kind='barh',color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression, random forest, multilayer perceptron, and adaboost outperform KNN, even with all of the attempted pre-processing so far.\n",
    "\n",
    "### Putting it all together\n",
    "Different combinations of these options can be strung together simultaneously to iterate over multiple models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import itertools\n",
    "\n",
    "estimators = ['knn','logistic_regression','svm',\n",
    "              'multilayer_perceptron','random_forest','adaboost']\n",
    "\n",
    "feature_interaction_options = [True,False]\n",
    "feature_selection_options = [None,'select_k_best']\n",
    "scaling_options = [None,'standard','normal','min_max','binary']\n",
    "transformations = [None,'pca']\n",
    "\n",
    "pipeline_steps = [feature_interaction_options,feature_selection_options,scaling_options,\n",
    "                  transformations,estimators]\n",
    "\n",
    "pipeline_options = list(itertools.product(*pipeline_steps))\n",
    "\n",
    "optimized_pipelines = {}\n",
    "\n",
    "for pipeline_step_combo in pipeline_options:\n",
    "    model_name = []\n",
    "    \n",
    "    feature_interactions = pipeline_step_combo[0]\n",
    "    \n",
    "    if feature_interactions:\n",
    "        model_name.append('interactions')\n",
    "        \n",
    "    feature_selection_type = pipeline_step_combo[1]\n",
    "    \n",
    "    if feature_selection_type:\n",
    "        model_name.append('select')\n",
    "    \n",
    "    scale_type = pipeline_step_combo[2]\n",
    "    \n",
    "    if scale_type:\n",
    "        model_name.append(scale_type)\n",
    "        \n",
    "    transform_type = pipeline_step_combo[3]\n",
    "    \n",
    "    if transform_type:\n",
    "        model_name.append(transform_type)\n",
    "        \n",
    "    estimator = pipeline_step_combo[4]\n",
    "    \n",
    "    model_name.append(estimator)\n",
    "    \n",
    "    model_name = '_'.join(model_name)\n",
    "    \n",
    "    print model_name\n",
    "    \n",
    "    # Set pipeline keyword arguments\n",
    "    optimized_pipeline_kwargs = {\n",
    "        'feature_selection_type': feature_selection_type,\n",
    "        'scale_type': scale_type,\n",
    "        'feature_interactions': feature_interactions,\n",
    "        'transform_type': transform_type\n",
    "        }\n",
    "\n",
    "    # Initialize pipeline\n",
    "    optimized_pipeline = ppl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "\n",
    "    # Set pipeline fitting parameters\n",
    "    fit_kwargs = {\n",
    "        'cv': 10,\n",
    "        'num_parameter_combos': None,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': None,\n",
    "        'suppress_output': True,\n",
    "        'use_default_param_dist': True,\n",
    "        'param_dist': None,\n",
    "        'test_size': 0.2 # 20% saved as test set\n",
    "    }\n",
    "\n",
    "    # Fit data\n",
    "    optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "\n",
    "    # Save optimized pipeline\n",
    "    optimized_pipelines[model_name] = optimized_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "pipeline_keys = optimized_pipelines.keys()\n",
    "\n",
    "test_scores = [optimized_pipelines[key].test_score_ for key in pipeline_keys]\n",
    "\n",
    "ax = pd.Series(test_scores,index=pipeline_keys).sort_values().plot(kind='barh',color='black',figsize=(10,40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print optimized_pipelines['min_max_pca_multilayer_perceptron']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(optimized_pipelines.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of 240 different possible pipelines, best pipeline, with a test score of 0.899, appears to be min-max scaling between 0 and 1 funneled into a PCA and then into a multilayer perceptron with one hidden layer of size 5.\n",
    "\n",
    "It took roughly 3 hours to find it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting survival with the optimal model\n",
    "All one has to do to make a prediction is use the .predict method of the pipeline in the .pipeline field.\n",
    "\n",
    "Here's an example of predicting whether I would survive on the Titanic. I'm 32, would probably have one family member with me, might be Pclass1 (I'd hope), male, have a Ph.D (if that's what they mean by Dr.). I'm using the median Fare for Pclass 1 and randomly chose a city to have embarked from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "personal_stats = [32,1,0,df[df['Pclass']==1]['Fare'].median(),0,0,1,1,0,1,0,0,0,0,0,0]\n",
    "\n",
    "zip(personal_stats,X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimized_pipelines['min_max_pca_multilayer_perceptron'].pipeline.predict(personal_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like I died!\n",
    "\n",
    "Let's look at my predicted probability of surviving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimized_pipelines['min_max_pca_multilayer_perceptron'].pipeline.predict_proba(personal_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would have a 0.77% chance of survival."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "I've shown how to use pyplearnr to try out 240 different pipeline combinations validated with stratified 10-folds cross-validation using a combination of simple keyword arguments with some additional customization options. Also, I've shown how to access the model parameters, predict survival, and check the actual predicted probability according to the optimized pipeline.\n",
    "\n",
    "Please let me know if you have any questions or suggestions about how to improve this tool, my code, the approach I'm taking, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pyplearnr as ppl\n",
    "\n",
    "repeated_k_folds = []\n",
    "\n",
    "for i in range(100):\n",
    "    # Alert user of step number\n",
    "    print('Step %d/%d'%(i+1,100))\n",
    "    \n",
    "    # Set custom parameters\n",
    "    param_dist = {}\n",
    "\n",
    "    estimator = 'knn'\n",
    "\n",
    "    # Initialize pipeline\n",
    "    optimized_pipeline = ppl.PipelineOptimization(estimator)\n",
    "\n",
    "    # Set pipeline fitting parameters\n",
    "    fit_kwargs = {\n",
    "        'use_default_param_dist': True,\n",
    "        'param_dist': param_dist,\n",
    "    }\n",
    "\n",
    "    # Fit data\n",
    "    optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "\n",
    "    # Save \n",
    "    repeated_k_folds.append(optimized_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    'train scores': [pipeline_optimization.train_score_ \n",
    "                     for pipeline_optimization in repeated_k_folds],\n",
    "    'test scores': [pipeline_optimization.test_score_ \n",
    "                     for pipeline_optimization in repeated_k_folds],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "repeated_kfcv_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "repeated_kfcv_df['test scores'].plot(kind='hist',bins=8,color='grey')\n",
    "repeated_kfcv_df['train scores'].plot(kind='hist',bins=8,color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reload(ppl)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pyplearnr as ppl\n",
    "\n",
    "repeated_five_folds = []\n",
    "\n",
    "for i in range(100):\n",
    "    # Alert user of step number\n",
    "    print('Step %d/%d'%(i+1,100))\n",
    "    \n",
    "    # Set custom parameters\n",
    "    param_dist = {}\n",
    "\n",
    "    estimator = 'knn'\n",
    "\n",
    "    # Initialize pipeline\n",
    "    optimized_pipeline = ppl.PipelineOptimization(estimator)\n",
    "\n",
    "    # Set pipeline fitting parameters\n",
    "    fit_kwargs = {\n",
    "        'use_default_param_dist': True,\n",
    "        'param_dist': param_dist,\n",
    "        'cv': 5,\n",
    "        'suppress_output': True\n",
    "    }\n",
    "\n",
    "    # Fit data\n",
    "    optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "\n",
    "    # Save \n",
    "    repeated_five_folds.append(optimized_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    'train scores': [pipeline_optimization.train_score_ \n",
    "                     for pipeline_optimization in repeated_five_folds],\n",
    "    'test scores': [pipeline_optimization.test_score_ \n",
    "                     for pipeline_optimization in repeated_five_folds],\n",
    "}\n",
    "\n",
    "repeated_fivefcv_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "repeated_kfcv_df['test scores'].plot(kind='hist',bins=8,color='grey')\n",
    "repeated_fivefcv_df['test scores'].plot(kind='hist',bins=8,color='red')\n",
    "repeated_kfcv_df['train scores'].plot(kind='hist',bins=8,color='white')\n",
    "\n",
    "repeated_fivefcv_df['train scores'].plot(kind='hist',bins=8,color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "repeated_fivefcv_df['test scores'].plot(kind='hist',bins=8,color='red')\n",
    "repeated_kfcv_df['test scores'].plot(kind='hist',bins=8,color='grey')\n",
    "repeated_kfcv_df['train scores'].plot(kind='hist',bins=8,color='white')\n",
    "\n",
    "repeated_fivefcv_df['train scores'].plot(kind='hist',bins=8,color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/Users/cmshymansky/documents/code/library/pairplotr')\n",
    "\n",
    "import pairplotr as ppr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "repeated_fivefcv_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(ppr)\n",
    "\n",
    "ppr.compare_data(repeated_fivefcv_df,bins=8,marker_size=10,plot_medians=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(ppr)\n",
    "\n",
    "ppr.compare_data(repeated_fivefcv_df,bins=8,marker_size=10,plot_medians=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "repeated_fivefcv_df['train scores'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pylab as plt\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "print ax\n",
    "\n",
    "# repeated_fivefcv_df.plot(ax=ax,x='train scores',y='test scores',style='bo')\n",
    "repeated_kfcv_df.plot(ax=ax,x='train scores',y='test scores',style='ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print dir(repeated_k_folds[0].grid_search)\n",
    "\n",
    "all_scores = []\n",
    "for x in repeated_k_folds[0].grid_search.grid_scores_:\n",
    "    all_scores.extend(list(x.cv_validation_scores))\n",
    "    print max(x.cv_validation_scores),x.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print repeated_k_folds[0].grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Series(all_scores).plot(kind='hist',color='grey',bins=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bootstrapped_datasets(orig_data_set, num_samples=100, points_per_sample=50):\n",
    "    import random\n",
    "\n",
    "    data_sets = []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        sample = [random.choice(orig_data_set) for x in range(points_per_sample)]\n",
    "        \n",
    "        data_sets.append(sample)\n",
    "\n",
    "    return data_sets\n",
    "\n",
    "def cdf(aList, x):\n",
    "    ''' 'aList' must be sorted (low to high) '''\n",
    "\n",
    "    returnVal=0\n",
    "    \n",
    "    for v in aList:\n",
    "        if v<=x:\n",
    "            returnVal+=1\n",
    "    \n",
    "    return returnVal/float(len(aList))\n",
    "\n",
    "def inv_cdf(aList, percentile):\n",
    "    ''' 'percentile' is between 0 and 1.\n",
    "        'aList' must be sorted (low to high)\n",
    "    '''\n",
    "    \n",
    "    returnVal = 0\n",
    "    \n",
    "    for i in xrange(len(aList)):\n",
    "        if cdf(aList, aList[i])>=percentile:\n",
    "            returnVal = aList[i]\n",
    "            break\n",
    "    \n",
    "    return returnVal\n",
    "\n",
    "def conf_interval(data_set, alpha=0.05):\n",
    "    data_set.sort()\n",
    "\n",
    "    low_end = inv_cdf(data_set, alpha)\n",
    "    high_end = inv_cdf(data_set, 1-alpha)\n",
    "\n",
    "    return (low_end, high_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pylab as plt\n",
    "\n",
    "bootstrapped_samples = get_bootstrapped_datasets(repeated_fivefcv_df['test scores'].values)\n",
    "\n",
    "avg_vals = [float(sum(l))/len(l) for l in bootstrapped_samples]\n",
    "\n",
    "conf_10000 = conf_interval(avg_vals)\n",
    "\n",
    "pd.Series(avg_vals).hist(bins=10, normed=True)\n",
    "\n",
    "plt.axvspan(conf_10000[0],conf_10000[1],alpha=0.5,color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.learning_curve import learning_curve\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "\n",
    "N, train_lc, val_lc = learning_curve(optimized_pipeline.pipeline,\n",
    "                                     X, y, cv=5,\n",
    "                                     train_sizes=np.linspace(0.3, 1, 25))\n",
    "\n",
    "ax.plot(N, np.mean(train_lc, 1), color='blue', label='training score')\n",
    "ax.plot(N, np.mean(val_lc, 1), color='red', label='validation score')\n",
    "ax.hlines(np.mean([train_lc[-1], val_lc[-1]]), N[0], N[-1],\n",
    "             color='gray', linestyle='dashed')\n",
    "\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlim(N[0], N[-1])\n",
    "ax.set_xlabel('training size')\n",
    "ax.set_ylabel('score')\n",
    "ax.legend(loc='best')\n",
    "\n",
    "\n",
    "# ax[i].plot(N, np.mean(train_lc, 1), color='blue', label='training score')\n",
    "# ax[i].plot(N, np.mean(val_lc, 1), color='red', label='validation score')\n",
    "# ax[i].hlines(np.mean([train_lc[-1], val_lc[-1]]), N[0], N[-1],\n",
    "#              color='gray', linestyle='dashed')\n",
    "\n",
    "# ax[i].set_ylim(0, 1)\n",
    "# ax[i].set_xlim(N[0], N[-1])\n",
    "# ax[i].set_xlabel('training size')\n",
    "# ax[i].set_ylabel('score')\n",
    "# ax[i].set_title('degree = {0}'.format(degree), size=14)\n",
    "# ax[i].legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set output feature\n",
    "output_feature = 'diabetes'\n",
    "\n",
    "# Get input features\n",
    "input_features = [x for x in X_interaction.columns if x != output_feature]\n",
    "\n",
    "# Split into features and responses\n",
    "X = X_interaction.copy()\n",
    "y = test_df[output_feature].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(ppl)\n",
    "\n",
    "ppl.OptimizationBundle().get_options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "estimator = 'knn'\n",
    "\n",
    "# Initialize pipeline\n",
    "optimized_pipeline = ppl.PipelineOptimization(estimator)\n",
    "\n",
    "# Fit data\n",
    "optimized_pipeline.fit(X,y,random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import sklearn.metrics as sklearn_metrics\n",
    "\n",
    "X_array = X.copy().values\n",
    "y_array = y.copy().values\n",
    "\n",
    "param_grid = {\n",
    "    'estimator__n_neighbors': range(31), \n",
    "    'estimator__weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "X_train, X_val, y_train, y_val = \\\n",
    "    train_test_split(X_array,y_array,test_size=0.2,random_state=6,stratify=y_array)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kfolds_kwargs = dict(\n",
    "    n_splits=10, \n",
    "    shuffle=True, \n",
    "    random_state=6    \n",
    ")\n",
    "\n",
    "skf = StratifiedKFold(**kfolds_kwargs)\n",
    "\n",
    "fold_optimizations = {}\n",
    "for fold_ind, data_inds in enumerate(skf.split(X_train, y_train)):\n",
    "    fold_optimizations[fold_ind] = {}\n",
    "    \n",
    "    train_index, test_index = data_inds[0],data_inds[1]\n",
    "    \n",
    "    X_train_inner, X_test_inner = X_array[train_index], X_array[test_index]\n",
    "    y_train_inner, y_test_inner = y_array[train_index], y_array[test_index]\n",
    "    \n",
    "    pipeline = Pipeline([('estimator',KNeighborsClassifier(n_neighbors=11,weights='distance'))])\n",
    "    \n",
    "    pipeline.fit(X_train_inner,y_train_inner)\n",
    "    \n",
    "    y_pred_inner = pipeline.predict(X_test_inner)\n",
    "    \n",
    "    confusion_matrix = sklearn_metrics.confusion_matrix(y_test_inner, y_pred_inner)\n",
    "    \n",
    "    score = confusion_matrix.trace()/float(confusion_matrix.sum())\n",
    "    \n",
    "    fold_optimizations[fold_ind]['confusion_matrix'] = confusion_matrix\n",
    "    \n",
    "    fold_optimizations[fold_ind]['score'] = confusion_matrix.trace()/float(confusion_matrix.sum())\n",
    "    fold_optimizations[fold_ind]['pipeline'] = pipeline\n",
    "    \n",
    "print np.array([fold_optimizations[fold_ind]['score'] for fold_ind in fold_optimizations]).mean()\n",
    "\n",
    "y_pred = pipeline.predict(X_val)\n",
    "\n",
    "test_confusion_matrix = sklearn_metrics.confusion_matrix(y_val, y_pred)\n",
    "\n",
    "score = test_confusion_matrix.trace()/float(test_confusion_matrix.sum())\n",
    "\n",
    "print score\n",
    "    \n",
    "# TRAIN: [1 3] TEST: [0 2]\n",
    "# TRAIN: [0 2] TEST: [1 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fold_optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print dir(optimized_pipeline.grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir(folds[0].named_steps['estimator'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
