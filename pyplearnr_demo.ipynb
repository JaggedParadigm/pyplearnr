{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyplearnr demo\n",
    "Here I demonstrate pyplearnr, a wrapper for building/training/validating scikit learn pipelines using GridSearchCV or RandomizedSearchCV.\n",
    "\n",
    "Quick keyword arguments give access to optional feature selection (e.g. SelectKBest), scaling (e.g. standard scaling), use of feature interactions, and data transformations (e.g. PCA, t-SNE) before being fed to a classifier/regressor.\n",
    "\n",
    "After building the pipeline, data can be used to perform a nested (stratified if classification) k-folds cross-validation and output an object containing data from the process, including the best model.\n",
    "\n",
    "Various default pipeline step parameters for the grid-search for quick iteration over different pipelines, with the option to ignore/override them in a flexible way.\n",
    "\n",
    "This is an on-going project that I intend to update with more models and pre-processing options and also with corresponding defaults.\n",
    "\n",
    "## Titanic dataset example\n",
    "Here I use the Titanic dataset I've cleaned and pickled in a separate tutorial.\n",
    "\n",
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 890 entries, 0 to 890\n",
      "Data columns (total 9 columns):\n",
      "Survived    890 non-null int64\n",
      "Pclass      890 non-null int64\n",
      "Sex         890 non-null object\n",
      "Age         890 non-null float64\n",
      "SibSp       890 non-null int64\n",
      "Parch       890 non-null int64\n",
      "Fare        890 non-null float64\n",
      "Embarked    890 non-null object\n",
      "Title       890 non-null object\n",
      "dtypes: float64(2), int64(4), object(3)\n",
      "memory usage: 69.5+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle('trimmed_titanic_data.pkl')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By \"cleaned\" I mean I've derived titles (e.g. \"Mr.\", \"Mrs.\", \"Dr.\", etc) from the passenger names, imputed the missing Age values using polynomial regression with grid-searched 10-fold cross-validation, filled in the 3 missing Embarked values with the mode, and removed all fields that could be considered an id for that individual.\n",
    "\n",
    "Thus, there is no missing data.\n",
    "\n",
    "## Set categorical features as type 'category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 890 entries, 0 to 890\n",
      "Data columns (total 9 columns):\n",
      "Survived    890 non-null category\n",
      "Pclass      890 non-null category\n",
      "Sex         890 non-null category\n",
      "Age         890 non-null float64\n",
      "SibSp       890 non-null int64\n",
      "Parch       890 non-null int64\n",
      "Fare        890 non-null float64\n",
      "Embarked    890 non-null category\n",
      "Title       890 non-null category\n",
      "dtypes: category(5), float64(2), int64(2)\n",
      "memory usage: 39.3 KB\n"
     ]
    }
   ],
   "source": [
    "simulation_df = df.copy()\n",
    "\n",
    "categorical_features = ['Survived','Pclass','Sex','Embarked','Title']\n",
    "\n",
    "for feature in categorical_features:\n",
    "    simulation_df[feature] = simulation_df[feature].astype('category')\n",
    "    \n",
    "simulation_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 890 entries, 0 to 890\n",
      "Data columns (total 17 columns):\n",
      "Age               890 non-null float64\n",
      "SibSp             890 non-null int64\n",
      "Parch             890 non-null int64\n",
      "Fare              890 non-null float64\n",
      "Survived_1        890 non-null uint8\n",
      "Pclass_2          890 non-null uint8\n",
      "Pclass_3          890 non-null uint8\n",
      "Sex_male          890 non-null uint8\n",
      "Embarked_Q        890 non-null uint8\n",
      "Embarked_S        890 non-null uint8\n",
      "Title_Dr          890 non-null uint8\n",
      "Title_Military    890 non-null uint8\n",
      "Title_Miss        890 non-null uint8\n",
      "Title_Mr          890 non-null uint8\n",
      "Title_Mrs         890 non-null uint8\n",
      "Title_Noble       890 non-null uint8\n",
      "Title_Rev         890 non-null uint8\n",
      "dtypes: float64(2), int64(2), uint8(13)\n",
      "memory usage: 46.1 KB\n"
     ]
    }
   ],
   "source": [
    "simulation_df = pd.get_dummies(simulation_df,drop_first=True)\n",
    "\n",
    "simulation_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 17 features.\n",
    "\n",
    "### Split into input/output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set output feature\n",
    "output_feature = 'Survived_1'\n",
    "\n",
    "# Get all column names\n",
    "column_names = list(simulation_df.columns)\n",
    "\n",
    "# Get input features\n",
    "input_features = [x for x in column_names if x != output_feature]\n",
    "\n",
    "# Split into features and responses\n",
    "X = simulation_df[input_features].copy()\n",
    "y = simulation_df[output_feature].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.61573034,  0.38426966])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulation_df['Survived_1'].value_counts().values/float(simulation_df['Survived_1'].value_counts().values.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, null accuracy of ~62% if always predict death.\n",
    "\n",
    "### Import data science library and initialize optimized pipeline collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyplearnr as ppl\n",
    "\n",
    "optimized_pipelines = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic models w/ no pre-processing\n",
    "#### KNN\n",
    "Here we do a simple K-nearest neighbors (KNN) classification with stratified 10-fold (default) cross-validation with a grid search over the default of 1 to 30 nearest neighbors and the use of either \"uniform\" or \"distance\" weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.15 s, sys: 95.7 ms, total: 2.24 s\n",
      "Wall time: 3.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "estimator = 'knn'\n",
    "\n",
    "# Set pipeline keyword arguments\n",
    "optimized_pipeline_kwargs = {\n",
    "    'feature_selection_type': None,\n",
    "    'scale_type': None,\n",
    "    'feature_interactions': False,\n",
    "    'transform_type': None\n",
    "    }\n",
    "\n",
    "# Initialize pipeline\n",
    "optimized_pipeline = ppl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "\n",
    "# Set pipeline fitting parameters\n",
    "fit_kwargs = {\n",
    "    'cv': 10,\n",
    "    'num_parameter_combos': None,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 6,\n",
    "    'suppress_output': True,\n",
    "    'use_default_param_dist': True,\n",
    "    'param_dist': None,\n",
    "    'test_size': 0.2 # 20% saved as test set\n",
    "}\n",
    "\n",
    "# Fit data\n",
    "optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "\n",
    "# Save \n",
    "optimized_pipelines[estimator] = optimized_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the train_model() method is an instance of my custom OptimizedPipeline class  containing all of the data associated with the nested stratified k-folds cross-validation.\n",
    "\n",
    "This includes the data, its test/train splits (based on the test_size percentage keyword argument), the GridSearchCV or RandomizedGridSearchCV object, the Pipeline object that has been retrained using all of the data with the best parameters, test/train scores, and validation metrics/reports.\n",
    "\n",
    "A report can be printed immediately after the fit by setting the suppress_output keyword argument to True. \n",
    "\n",
    "It lists the steps in the pipeline, their optimized settings, the test/training accuracy (or L2 regression score), the grid search parameters, and the best parameters.\n",
    "\n",
    "If the estimator used is a classifier it also includes the confusion matrix, normalized confusion matrix, and a classification report containing precision/recall/f1-score for each class.\n",
    "\n",
    "This same report is also accessible by printing the OptimizedPipeline class instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline:\n",
      "\n",
      "1: estimator   KNeighborsClassifier           \n",
      "                                              \n",
      "               n_neighbors          = 12        \n",
      "               n_jobs               = 1         \n",
      "               algorithm            = auto      \n",
      "               metric               = minkowski \n",
      "               metric_params        = None      \n",
      "               p                    = 2         \n",
      "               weights              = uniform   \n",
      "               leaf_size            = 30        \n",
      "                                              \n",
      "\n",
      "Training set classification accuracy:\t0.729\n",
      "Test set classification accuracy:\t0.736\n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "[[99  7]\n",
      " [40 32]]\n",
      "\n",
      "Normalized confusion matrix:\n",
      "\n",
      "[[ 0.55617978  0.03932584]\n",
      " [ 0.2247191   0.17977528]]\n",
      "\n",
      "Classification report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.93      0.81       106\n",
      "          1       0.82      0.44      0.58        72\n",
      "\n",
      "avg / total       0.76      0.74      0.71       178\n",
      "\n",
      "\n",
      "Grid search parameters:\n",
      "\n",
      "               scoring              = accuracy  \n",
      "               n_jobs               = -1        \n",
      "               verbose              = 0         \n",
      "               fit_params           = {}        \n",
      "               refit                = 1         \n",
      "               return_train_score   = 1         \n",
      "               iid                  = 1         \n",
      "               error_score          = raise     \n",
      "               pre_dispatch         = 2*n_jobs  \n",
      "               param_grid           = {'estimator__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30], 'estimator__weights': ['uniform', 'distance']}\n",
      "               cv                   = 10        \n",
      "                                              \n",
      "Best parameters:                               \n",
      "                                              \n",
      "               estimator__n_neighbors = 12        \n",
      "               estimator__weights   = uniform   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print optimized_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out that the best settings are 12 neighbors and the use of the 'uniform' weight.\n",
    "\n",
    "Note how I've set the random_state keyword agument to 6 so that the models can be compared using the same test/train split.\n",
    "\n",
    "The default parameters to grid-search over for k-nearest neighbors is 1 to 30 neighbors and either the 'uniform' or 'distance' weight.\n",
    "\n",
    "The defaults for the pre-processing steps, classifiers, and regressors can be viewed by using the get_default_pipeline_step_parameters() method with the number of features as the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': [1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30],\n",
       " 'weights': ['uniform', 'distance']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_processing_grid_parameters,classifier_grid_parameters,regression_grid_parameters = \\\n",
    "optimized_pipeline.get_default_pipeline_step_parameters(X.shape[0])\n",
    "\n",
    "classifier_grid_parameters['knn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These default parameters can be ignored by setting the use_default_param_dist keyword argument to False. \n",
    "\n",
    "The param_dist keyword argument can be used to keep default parameters (if use_default_param_dist set to True) or to be used as the sole source of parameters (if use_default_param_dist set to False). \n",
    "\n",
    "Here is a demonstration of generation of default parameters with those in param_dist being overridden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0d8c0a631c08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu\"\\nreload(ppl)\\nestimator_name = 'knn'\\n\\nmodel_name = 'custom_override_%s'%(estimator_name)\\n\\n# Set custom parameters\\nparam_dist = {\\n    'estimator__n_neighbors': range(30,500)\\n}\\n\\n# Set pipeline keyword arguments\\noptimized_pipeline_kwargs = {\\n    'feature_selection_type': None,\\n    'scale_type': None,\\n    'feature_interactions': False,\\n    'transform_type': None\\n    }\\n\\n# Initialize pipeline\\noptimized_pipeline = ppl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\\n\\n# Set pipeline fitting parameters\\nfit_kwargs = {\\n    'cv': 10,\\n    'num_parameter_combos': None,\\n    'n_jobs': -1,\\n    'random_state': 6,\\n    'suppress_output': False,\\n    'use_default_param_dist': True,\\n    'param_dist': param_dist,\\n    'test_size': 0.2 # 20% saved as test set\\n}\\n\\n# Fit data\\noptimized_pipeline.fit(X,y,**fit_kwargs)\\n\\n# Save \\noptimized_pipelines[model_name] = optimized_pipeline\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/cmshymansky/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/cmshymansky/anaconda/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cmshymansky/anaconda/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/cmshymansky/Documents/code/machine_learning/tools/pyplearnr/pyplearnr.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cv, num_parameter_combos, n_jobs, random_state, suppress_output, use_default_param_dist, param_dist, test_size)\u001b[0m\n\u001b[1;32m    468\u001b[0m                                                   \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m                                                   \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m                                                   num_parameter_combos=num_parameter_combos)\n\u001b[0m\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0;31m# Save pipeline metrics and predicated values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cmshymansky/Documents/code/machine_learning/tools/pyplearnr/pyplearnr.py\u001b[0m in \u001b[0;36moptimize_pipeline\u001b[0;34m(self, X_train, y_train, pipeline, scoring, param_dist, cv, n_jobs, num_parameter_combos)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;31m# Perform grid search using above parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yay'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;31m# Return grid_search object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cmshymansky/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtest\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \"\"\"\n\u001b[0;32m--> 940\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cmshymansky/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, groups, parameter_iterable)\u001b[0m\n\u001b[1;32m    560\u001b[0m                                   \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m--> 562\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m           for train, test in cv.split(X, y, groups))\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cmshymansky/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cmshymansky/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "reload(ppl)\n",
    "estimator_name = 'knn'\n",
    "\n",
    "model_name = 'custom_override_%s'%(estimator_name)\n",
    "\n",
    "# Set custom parameters\n",
    "param_dist = {\n",
    "    'estimator__n_neighbors': range(30,500)\n",
    "}\n",
    "\n",
    "# Set pipeline keyword arguments\n",
    "optimized_pipeline_kwargs = {\n",
    "    'feature_selection_type': None,\n",
    "    'scale_type': None,\n",
    "    'feature_interactions': False,\n",
    "    'transform_type': None\n",
    "    }\n",
    "\n",
    "# Initialize pipeline\n",
    "optimized_pipeline = ppl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "\n",
    "# Set pipeline fitting parameters\n",
    "fit_kwargs = {\n",
    "    'cv': 10,\n",
    "    'num_parameter_combos': None,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 6,\n",
    "    'suppress_output': False,\n",
    "    'use_default_param_dist': True,\n",
    "    'param_dist': param_dist,\n",
    "    'test_size': 0.2 # 20% saved as test set\n",
    "}\n",
    "\n",
    "# Fit data\n",
    "optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "\n",
    "# Save \n",
    "optimized_pipelines[model_name] = optimized_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the n_neighbors parameter was 30 to 499 instead of 1 to 30.\n",
    "\n",
    "Here's an example of only using param_dist for parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reload(dsl)\n",
    "\n",
    "model_name = 'from_scratch_%s'%(estimator_name)\n",
    "\n",
    "# Set custom parameters\n",
    "param_dist = {\n",
    "    'estimator__n_neighbors': range(10,30)\n",
    "}\n",
    "\n",
    "estimator = 'knn'\n",
    "\n",
    "# Set pipeline keyword arguments\n",
    "optimized_pipeline_kwargs = {\n",
    "    'feature_selection_type': None,\n",
    "    'scale_type': None,\n",
    "    'feature_interactions': False,\n",
    "    'transform_type': None\n",
    "    }\n",
    "\n",
    "# Initialize pipeline\n",
    "optimized_pipeline = dsl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "\n",
    "# Set pipeline fitting parameters\n",
    "fit_kwargs = {\n",
    "    'cv': 10,\n",
    "    'num_parameter_combos': None,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 6,\n",
    "    'suppress_output': False,\n",
    "    'use_default_param_dist': False,\n",
    "    'param_dist': param_dist,\n",
    "    'test_size': 0.2 # 20% saved as test set\n",
    "}\n",
    "\n",
    "# Fit data\n",
    "optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "\n",
    "# Save \n",
    "optimized_pipelines[model_name] = optimized_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the estimator\\_\\_weights parameter isn't set for the KNN estimator.\n",
    "\n",
    "#### Other models\n",
    "\n",
    "This code currently supports K-nearest neighbors, logistic regression, support vector machines, multilayer perceptrons, random forest, and adaboost.\n",
    "We can loop through and pick the best model like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reload(dsl)\n",
    "\n",
    "classifiers = ['knn','logistic_regression','svm',\n",
    "               'multilayer_perceptron','random_forest','adaboost']\n",
    "\n",
    "for estimator in classifiers:\n",
    "    # Set pipeline keyword arguments\n",
    "    optimized_pipeline_kwargs = {\n",
    "        'feature_selection_type': None,\n",
    "        'scale_type': None,\n",
    "        'feature_interactions': False,\n",
    "        'transform_type': None\n",
    "        }\n",
    "\n",
    "    # Initialize pipeline\n",
    "    optimized_pipeline = dsl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "    \n",
    "    # Set pipeline fitting parameters\n",
    "    fit_kwargs = {\n",
    "        'cv': 10,\n",
    "        'num_parameter_combos': None,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 6,\n",
    "        'suppress_output': True,\n",
    "        'use_default_param_dist': True,\n",
    "        'param_dist': None,\n",
    "        'test_size': 0.2\n",
    "    }\n",
    "    \n",
    "    # Fit data\n",
    "    optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "    \n",
    "    # Save \n",
    "    optimized_pipelines[estimator] = optimized_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "format_str = '{0:<22} {1:<15} {2:<15}'\n",
    "\n",
    "print format_str.format(*['model','train score','test score'])\n",
    "print format_str.format(*['','',''])\n",
    "for x in [[key,value.train_score_,value.test_score_] for key,value in optimized_pipelines.iteritems()]:\n",
    "    print format_str.format(*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest performed the best with a test score of ~0.854.\n",
    "\n",
    "Lets look at the report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print optimized_pipelines['random_forest']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal parameter was 96 for the n_estimators parameter for the RandomizedForestClassifier.\n",
    "\n",
    "### All models with standard scaling\n",
    "We can set the scaling type using the scale_type keyword argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reload(dsl)\n",
    "\n",
    "classifiers = ['knn','logistic_regression','svm',\n",
    "               'multilayer_perceptron','random_forest','adaboost']\n",
    "\n",
    "prefix = 'scale'\n",
    "\n",
    "for estimator in classifiers:\n",
    "    # Set pipeline keyword arguments\n",
    "    optimized_pipeline_kwargs = {\n",
    "        'feature_selection_type': None,\n",
    "        'scale_type': 'standard',\n",
    "        'feature_interactions': False,\n",
    "        'transform_type': None\n",
    "        }\n",
    "\n",
    "    # Initialize pipeline\n",
    "    optimized_pipeline = dsl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "    \n",
    "    # Set pipeline fitting parameters\n",
    "    fit_kwargs = {\n",
    "        'cv': 10,\n",
    "        'num_parameter_combos': None,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 6,\n",
    "        'suppress_output': True,\n",
    "        'use_default_param_dist': True,\n",
    "        'param_dist': None,\n",
    "        'test_size': 0.2\n",
    "    }\n",
    "    \n",
    "    # Fit data\n",
    "    optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "    \n",
    "    # Form name used to save optimized pipeline\n",
    "    pipeline_name = '%s_%s'%(prefix,estimator)\n",
    "    \n",
    "    # Save \n",
    "    optimized_pipelines[pipeline_name] = optimized_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "format_str = '{0:<30} {1:<15} {2:<15}'\n",
    "\n",
    "print format_str.format(*['model','train score','test score'])\n",
    "print format_str.format(*['','',''])\n",
    "for x in [[key,value.train_score_,value.test_score_] for key,value in optimized_pipelines.iteritems()]:\n",
    "    print format_str.format(*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest without scaling still appears to have the best test score. Though that with scaling had closer test and train scores.\n",
    "\n",
    "### All models with SelectKBest feature selection\n",
    "Setting the feature_selection_type keyword argument will use SelectKBest with f_classif for feature selection: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reload(dsl)\n",
    "\n",
    "classifiers = ['knn','logistic_regression','svm',\n",
    "               'multilayer_perceptron','random_forest','adaboost']\n",
    "\n",
    "prefix = 'select'\n",
    "\n",
    "for estimator in classifiers:\n",
    "    # Set pipeline keyword arguments\n",
    "    optimized_pipeline_kwargs = {\n",
    "        'feature_selection_type': 'select_k_best',\n",
    "        'scale_type': None,\n",
    "        'feature_interactions': False,\n",
    "        'transform_type': None\n",
    "        }\n",
    "\n",
    "    # Initialize pipeline\n",
    "    optimized_pipeline = dsl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "    \n",
    "    # Set pipeline fitting parameters\n",
    "    fit_kwargs = {\n",
    "        'cv': 10,\n",
    "        'num_parameter_combos': None,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 6,\n",
    "        'suppress_output': True,\n",
    "        'use_default_param_dist': True,\n",
    "        'param_dist': None,\n",
    "        'test_size': 0.2\n",
    "    }\n",
    "    \n",
    "    # Fit data\n",
    "    optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "    \n",
    "    # Form name used to save optimized pipeline\n",
    "    pipeline_name = '%s_%s'%(prefix,estimator)\n",
    "    \n",
    "    # Save \n",
    "    optimized_pipelines[pipeline_name] = optimized_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "format_str = '{0:<30} {1:<15} {2:<15} {3:<15}'\n",
    "\n",
    "print format_str.format(*['model','train score','test score','train-test'])\n",
    "print format_str.format(*['','','',''])\n",
    "for x in [[key,value.train_score_,value.test_score_,value.train_score_-value.test_score_] for key,value in optimized_pipelines.iteritems()]:\n",
    "    print format_str.format(*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, random_forest performs the best. \n",
    "\n",
    "Though K-nearest neighbors appears to have the smallest difference between testing and training sets.\n",
    "\n",
    "### All models with feature interaction\n",
    "Setting the feature_interactions keyword argument to True will cause the use of feature interactions. The default is to only consider pairwise products, though this be set to higher by overriding using param_dist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reload(dsl)\n",
    "\n",
    "classifiers = ['knn','logistic_regression','svm','multilayer_perceptron','random_forest','adaboost']\n",
    "\n",
    "prefix = 'interact'\n",
    "\n",
    "for estimator in classifiers:\n",
    "    # Set pipeline keyword arguments\n",
    "    optimized_pipeline_kwargs = {\n",
    "        'feature_selection_type': None,\n",
    "        'scale_type': None,\n",
    "        'feature_interactions': True,\n",
    "        'transform_type': None\n",
    "        }\n",
    "\n",
    "    # Initialize pipeline\n",
    "    optimized_pipeline = dsl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "    \n",
    "    # Set pipeline fitting parameters\n",
    "    fit_kwargs = {\n",
    "        'cv': 10,\n",
    "        'num_parameter_combos': None,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 6,\n",
    "        'suppress_output': True,\n",
    "        'use_default_param_dist': True,\n",
    "        'param_dist': None,\n",
    "        'test_size': 0.2\n",
    "    }\n",
    "    \n",
    "    # Fit data\n",
    "    optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "    \n",
    "    # Form name used to save optimized pipeline\n",
    "    pipeline_name = '%s_%s'%(prefix,estimator)\n",
    "    \n",
    "    # Save \n",
    "    optimized_pipelines[pipeline_name] = optimized_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "format_str = '{0:<30} {1:<15} {2:<15} {3:<15}'\n",
    "\n",
    "print format_str.format(*['model','train score','test score','train-test'])\n",
    "print format_str.format(*['','','',''])\n",
    "for x in [[key,value.train_score_,value.test_score_,value.train_score_-value.test_score_] \\\n",
    "          for key,value in optimized_pipelines.iteritems()]:\n",
    "    print format_str.format(*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't appear to result in many gains in this case.\n",
    "\n",
    "### All models with transformed data\n",
    "Setting the transform_type to 'pca' or 't-sne' will apply Principal Component Analysis or t-distributed stochastic neighbor embedding, respectively, to the data before applying the estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reload(dsl)\n",
    "\n",
    "classifiers = ['knn','logistic_regression','svm',\n",
    "               'multilayer_perceptron','random_forest','adaboost']\n",
    "\n",
    "prefix = 'pca'\n",
    "\n",
    "for estimator in classifiers:\n",
    "    # Set pipeline keyword arguments\n",
    "    optimized_pipeline_kwargs = {\n",
    "        'feature_selection_type': None,\n",
    "        'scale_type': None,\n",
    "        'feature_interactions': None,\n",
    "        'transform_type': 'pca'\n",
    "        }\n",
    "\n",
    "    # Initialize pipeline\n",
    "    optimized_pipeline = dsl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "    \n",
    "    # Set pipeline fitting parameters\n",
    "    fit_kwargs = {\n",
    "        'cv': 10,\n",
    "        'num_parameter_combos': None,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 6,\n",
    "        'suppress_output': True,\n",
    "        'use_default_param_dist': True,\n",
    "        'param_dist': None,\n",
    "        'test_size': 0.2\n",
    "    }\n",
    "    \n",
    "    # Fit data\n",
    "    optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "    \n",
    "    # Form name used to save optimized pipeline\n",
    "    pipeline_name = '%s_%s'%(prefix,estimator)\n",
    "    \n",
    "    # Save \n",
    "    optimized_pipelines[pipeline_name] = optimized_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "format_str = '{0:<30} {1:<15} {2:<15} {3:<15}'\n",
    "\n",
    "print format_str.format(*['model','train score','test score','train-test'])\n",
    "print format_str.format(*['','','',''])\n",
    "for x in [[key,value.train_score_,value.test_score_,value.train_score_-value.test_score_] for key,value in optimized_pipelines.iteritems()]:\n",
    "    print format_str.format(*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the use of t-SNE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reload(dsl)\n",
    "\n",
    "classifiers = ['knn','logistic_regression','svm','multilayer_perceptron','random_forest','adaboost']\n",
    "\n",
    "prefix = 't_sne'\n",
    "\n",
    "for estimator in classifiers:\n",
    "    # Set pipeline keyword arguments\n",
    "    optimized_pipeline_kwargs = {\n",
    "        'feature_selection_type': None,\n",
    "        'scale_type': None,\n",
    "        'feature_interactions': None,\n",
    "        'transform_type': 't-sne'\n",
    "        }\n",
    "\n",
    "    # Initialize pipeline\n",
    "    optimized_pipeline = dsl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "    \n",
    "    # Set pipeline fitting parameters\n",
    "    fit_kwargs = {\n",
    "        'cv': 10,\n",
    "        'num_parameter_combos': None,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 6,\n",
    "        'suppress_output': True,\n",
    "        'use_default_param_dist': True,\n",
    "        'param_dist': None,\n",
    "        'test_size': 0.2\n",
    "    }\n",
    "    \n",
    "    # Fit data\n",
    "    optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "    \n",
    "    # Form name used to save optimized pipeline\n",
    "    pipeline_name = '%s_%s'%(prefix,estimator)\n",
    "    \n",
    "    # Save \n",
    "    optimized_pipelines[pipeline_name] = optimized_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "format_str = '{0:<30} {1:<15} {2:<15} {3:<15}'\n",
    "\n",
    "print format_str.format(*['model','train score','test score','train-test'])\n",
    "print format_str.format(*['','','',''])\n",
    "for x in [[key,value.train_score_,value.test_score_,value.train_score_-value.test_score_] for key,value in optimized_pipelines.iteritems()]:\n",
    "    print format_str.format(*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that took forever.\n",
    "\n",
    "We can get a better idea on how long this will take by setting the num_parameter_combos keyword argument. Setting this will only allow that number of grid combinations to be used for each run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reload(dsl)\n",
    "\n",
    "classifiers = ['knn','logistic_regression','svm',\n",
    "               'multilayer_perceptron','random_forest','adaboost']\n",
    "\n",
    "prefix = 't_sne_less_combo'\n",
    "\n",
    "for estimator in classifiers:\n",
    "    # Set pipeline keyword arguments\n",
    "    optimized_pipeline_kwargs = {\n",
    "        'feature_selection_type': None,\n",
    "        'scale_type': None,\n",
    "        'feature_interactions': None,\n",
    "        'transform_type': 't-sne'\n",
    "        }\n",
    "\n",
    "    # Initialize pipeline\n",
    "    optimized_pipeline = dsl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "    \n",
    "    # Set pipeline fitting parameters\n",
    "    fit_kwargs = {\n",
    "        'cv': 10,\n",
    "        'num_parameter_combos': 1,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 6,\n",
    "        'suppress_output': True,\n",
    "        'use_default_param_dist': True,\n",
    "        'param_dist': None,\n",
    "        'test_size': 0.2\n",
    "    }\n",
    "    \n",
    "    # Fit data\n",
    "    optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "    \n",
    "    # Form name used to save optimized pipeline\n",
    "    pipeline_name = '%s_%s'%(prefix,estimator)\n",
    "    \n",
    "    # Save \n",
    "    optimized_pipelines[pipeline_name] = optimized_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying t-sne to the data and then testing the 6 classifiers takes about 7 min. This could be optimized by pre-transforming the data once and then applying the classifiers. I'm thinking of creating some sort of container class that should be able to optimize this in the future.\n",
    "\n",
    "### SelectKBest, standard scaling, and all classifiers\n",
    "Finally, here we appply feature selection and standard scaling for all 6 classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reload(dsl)\n",
    "\n",
    "classifiers = ['knn','logistic_regression','svm',\n",
    "               'multilayer_perceptron','random_forest','adaboost']\n",
    "\n",
    "prefix = 'select_standard'\n",
    "\n",
    "for estimator in classifiers:\n",
    "    # Set pipeline keyword arguments\n",
    "    optimized_pipeline_kwargs = {\n",
    "        'feature_selection_type': 'select_k_best',\n",
    "        'scale_type': 'standard',\n",
    "        'feature_interactions': None,\n",
    "        'transform_type': None\n",
    "        }\n",
    "\n",
    "    # Initialize pipeline\n",
    "    optimized_pipeline = dsl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "    \n",
    "    # Set pipeline fitting parameters\n",
    "    fit_kwargs = {\n",
    "        'cv': 10,\n",
    "        'num_parameter_combos': None,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 6,\n",
    "        'suppress_output': True,\n",
    "        'use_default_param_dist': True,\n",
    "        'param_dist': None,\n",
    "        'test_size': 0.2\n",
    "    }\n",
    "    \n",
    "    # Fit data\n",
    "    optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "    \n",
    "    # Form name used to save optimized pipeline\n",
    "    pipeline_name = '%s_%s'%(prefix,estimator)\n",
    "    \n",
    "    # Save \n",
    "    optimized_pipelines[pipeline_name] = optimized_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "format_str = '{0:<40} {1:<15} {2:<15} {3:<15}'\n",
    "\n",
    "print format_str.format(*['model','train score','test score','train-test'])\n",
    "print format_str.format(*['','','',''])\n",
    "for x in [[key,value.train_score_,value.test_score_,value.train_score_-value.test_score_] for key,value in optimized_pipelines.iteritems()]:\n",
    "    print format_str.format(*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(optimized_pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 48 different pre-processing/transformation/classification combinations, this has become rather unwieldy. \n",
    "\n",
    "Here I make a quick dataframe of the test/train scores and visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "model_indices = optimized_pipelines.keys()\n",
    "train_scores = [value.train_score_ for key,value in optimized_pipelines.iteritems()]\n",
    "test_scores = [value.test_score_ for key,value in optimized_pipelines.iteritems()]\n",
    "\n",
    "score_df = pd.DataFrame({'training_score':train_scores,'test_score':test_scores},\n",
    "                        index=model_indices)\n",
    "\n",
    "score_df['test-train'] = score_df['test_score']-score_df['training_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_df['test_score'].sort_values().plot(kind='barh',figsize=(10,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best training score was acheived by the random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_df['test-train'].sort_values().plot(kind='barh',figsize=(10,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = score_df.plot(x=['test_score'],y='test-train',style='o',legend=None)\n",
    "\n",
    "ax.set_xlabel('test score')\n",
    "ax.set_ylabel('test-train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the best model was random forest.\n",
    "\n",
    "Here's the report for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print optimized_pipelines['random_forest']"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
