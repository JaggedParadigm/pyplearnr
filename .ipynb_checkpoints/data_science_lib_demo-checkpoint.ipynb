{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyplearnr demo\n",
    "Here I demonstrate pyplearnr, a wrapper for building/training/validating scikit learn pipelines using GridSearchCV or RandomizedSearchCV.\n",
    "\n",
    "Quick keyword arguments give access to optional feature selection (e.g. SelectKBest), scaling (e.g. standard scaling), use of feature interactions, and data transformations (e.g. PCA, t-SNE) before being fed to a classifier/regressor.\n",
    "\n",
    "After building the pipeline, data can be used to perform a nested (stratified if classification) k-folds cross-validation and output an object containing data from the process, including the best model.\n",
    "\n",
    "Various default pipeline step parameters for the grid-search for quick iteration over different pipelines, with the option to ignore/override them in a flexible way.\n",
    "\n",
    "This is an on-going project that I intend to update with more models and pre-processing options and also with corresponding defaults.\n",
    "\n",
    "## Titanic dataset example\n",
    "Here I use the Titanic dataset I've cleaned and pickled in a separate tutorial.\n",
    "\n",
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle('trimmed_titanic_data.pkl')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By \"cleaned\" I mean I've derived titles (e.g. \"Mr.\", \"Mrs.\", \"Dr.\", etc) from the passenger names, imputed the missing Age values using polynomial regression with grid-searched 10-fold cross-validation, filled in the 3 missing Embarked values with the mode, and removed all fields that could be considered an id for that individual.\n",
    "\n",
    "Thus, there is no missing data.\n",
    "\n",
    "## Set categorical features as type 'category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simulation_df = df.copy()\n",
    "\n",
    "categorical_features = ['Survived','Pclass','Sex','Embarked','Title']\n",
    "\n",
    "for feature in categorical_features:\n",
    "    simulation_df[feature] = simulation_df[feature].astype('category')\n",
    "    \n",
    "simulation_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simulation_df = pd.get_dummies(simulation_df,drop_first=True)\n",
    "\n",
    "simulation_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 17 features.\n",
    "\n",
    "### Split into input/output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set output feature\n",
    "output_feature = 'Survived_1'\n",
    "\n",
    "# Get all column names\n",
    "column_names = list(simulation_df.columns)\n",
    "\n",
    "# Get input features\n",
    "input_features = [x for x in column_names if x != output_feature]\n",
    "\n",
    "# Split into features and responses\n",
    "X = simulation_df[input_features].copy()\n",
    "y = simulation_df[output_feature].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simulation_df['Survived_1'].value_counts().values/float(simulation_df['Survived_1'].value_counts().values.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, null accuracy of ~62% if always predict death.\n",
    "\n",
    "### Import data science library and initialize optimized pipeline collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyplearnr as ppl\n",
    "\n",
    "optimized_pipelines = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic models w/ no pre-processing\n",
    "#### KNN\n",
    "Here we do a simple K-nearest neighbors (KNN) classification with stratified 10-fold (default) cross-validation with a grid search over the default of 1 to 30 nearest neighbors and the use of either \"uniform\" or \"distance\" weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reload(dsl)\n",
    "\n",
    "estimator = 'knn'\n",
    "\n",
    "# Set pipeline keyword arguments\n",
    "optimized_pipeline_kwargs = {\n",
    "    'feature_selection_type': None,\n",
    "    'scale_type': None,\n",
    "    'feature_interactions': False,\n",
    "    'transform_type': None\n",
    "    }\n",
    "\n",
    "# Initialize pipeline\n",
    "optimized_pipeline = ppl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "\n",
    "# Set pipeline fitting parameters\n",
    "fit_kwargs = {\n",
    "    'cv': 10,\n",
    "    'num_parameter_combos': None,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 6,\n",
    "    'suppress_output': True,\n",
    "    'use_default_param_dist': True,\n",
    "    'param_dist': None,\n",
    "    'test_size': 0.2 # 20% saved as test set\n",
    "}\n",
    "\n",
    "# Fit data\n",
    "optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "\n",
    "# Save \n",
    "optimized_pipelines[estimator] = optimized_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the train_model() method is an instance of my custom OptimizedPipeline class  containing all of the data associated with the nested stratified k-folds cross-validation.\n",
    "\n",
    "This includes the data, its test/train splits (based on the test_size percentage keyword argument), the GridSearchCV or RandomizedGridSearchCV object, the Pipeline object that has been retrained using all of the data with the best parameters, test/train scores, and validation metrics/reports.\n",
    "\n",
    "A report can be printed immediately after the fit by setting the suppress_output keyword argument to True. \n",
    "\n",
    "It lists the steps in the pipeline, their optimized settings, the test/training accuracy (or L2 regression score), the grid search parameters, and the best parameters.\n",
    "\n",
    "If the estimator used is a classifier it also includes the confusion matrix, normalized confusion matrix, and a classification report containing precision/recall/f1-score for each class.\n",
    "\n",
    "This same report is also accessible by printing the OptimizedPipeline class instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print optimized_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out that the best settings are 12 neighbors and the use of the 'uniform' weight.\n",
    "\n",
    "Note how I've set the random_state keyword agument to 6 so that the models can be compared using the same test/train split.\n",
    "\n",
    "The default parameters to grid-search over for k-nearest neighbors is 1 to 30 neighbors and either the 'uniform' or 'distance' weight.\n",
    "\n",
    "The defaults for the pre-processing steps, classifiers, and regressors can be viewed by using the get_default_pipeline_step_parameters() method with the number of features as the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pre_processing_grid_parameters,classifier_grid_parameters,regression_grid_parameters = \\\n",
    "optimized_pipeline.get_default_pipeline_step_parameters(X.shape[0])\n",
    "\n",
    "classifier_grid_parameters['knn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These default parameters can be ignored by setting the use_default_param_dist keyword argument to False. \n",
    "\n",
    "The param_dist keyword argument can be used to keep default parameters (if use_default_param_dist set to True) or to be used as the sole source of parameters (if use_default_param_dist set to False). \n",
    "\n",
    "Here is a demonstration of generation of default parameters with those in param_dist being overridden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reload(dsl)\n",
    "\n",
    "model_name = 'custom_override_%s'%(estimator_name)\n",
    "\n",
    "# Set custom parameters\n",
    "param_dist = {\n",
    "    'estimator__n_neighbors': range(30,500)\n",
    "}\n",
    "\n",
    "estimator = 'knn'\n",
    "\n",
    "# Set pipeline keyword arguments\n",
    "optimized_pipeline_kwargs = {\n",
    "    'feature_selection_type': None,\n",
    "    'scale_type': None,\n",
    "    'feature_interactions': False,\n",
    "    'transform_type': None\n",
    "    }\n",
    "\n",
    "# Initialize pipeline\n",
    "optimized_pipeline = dsl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "\n",
    "# Set pipeline fitting parameters\n",
    "fit_kwargs = {\n",
    "    'cv': 10,\n",
    "    'num_parameter_combos': None,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 6,\n",
    "    'suppress_output': False,\n",
    "    'use_default_param_dist': True,\n",
    "    'param_dist': param_dist,\n",
    "    'test_size': 0.2 # 20% saved as test set\n",
    "}\n",
    "\n",
    "# Fit data\n",
    "optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "\n",
    "# Save \n",
    "optimized_pipelines[model_name] = optimized_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the n_neighbors parameter was 30 to 499 instead of 1 to 30.\n",
    "\n",
    "Here's an example of only using param_dist for parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reload(dsl)\n",
    "\n",
    "model_name = 'from_scratch_%s'%(estimator_name)\n",
    "\n",
    "# Set custom parameters\n",
    "param_dist = {\n",
    "    'estimator__n_neighbors': range(10,30)\n",
    "}\n",
    "\n",
    "estimator = 'knn'\n",
    "\n",
    "# Set pipeline keyword arguments\n",
    "optimized_pipeline_kwargs = {\n",
    "    'feature_selection_type': None,\n",
    "    'scale_type': None,\n",
    "    'feature_interactions': False,\n",
    "    'transform_type': None\n",
    "    }\n",
    "\n",
    "# Initialize pipeline\n",
    "optimized_pipeline = dsl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "\n",
    "# Set pipeline fitting parameters\n",
    "fit_kwargs = {\n",
    "    'cv': 10,\n",
    "    'num_parameter_combos': None,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 6,\n",
    "    'suppress_output': False,\n",
    "    'use_default_param_dist': False,\n",
    "    'param_dist': param_dist,\n",
    "    'test_size': 0.2 # 20% saved as test set\n",
    "}\n",
    "\n",
    "# Fit data\n",
    "optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "\n",
    "# Save \n",
    "optimized_pipelines[model_name] = optimized_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the estimator\\_\\_weights parameter isn't set for the KNN estimator.\n",
    "\n",
    "#### Other models\n",
    "\n",
    "This code currently supports K-nearest neighbors, logistic regression, support vector machines, multilayer perceptrons, random forest, and adaboost.\n",
    "We can loop through and pick the best model like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reload(dsl)\n",
    "\n",
    "classifiers = ['knn','logistic_regression','svm',\n",
    "               'multilayer_perceptron','random_forest','adaboost']\n",
    "\n",
    "for estimator in classifiers:\n",
    "    # Set pipeline keyword arguments\n",
    "    optimized_pipeline_kwargs = {\n",
    "        'feature_selection_type': None,\n",
    "        'scale_type': None,\n",
    "        'feature_interactions': False,\n",
    "        'transform_type': None\n",
    "        }\n",
    "\n",
    "    # Initialize pipeline\n",
    "    optimized_pipeline = dsl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "    \n",
    "    # Set pipeline fitting parameters\n",
    "    fit_kwargs = {\n",
    "        'cv': 10,\n",
    "        'num_parameter_combos': None,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 6,\n",
    "        'suppress_output': True,\n",
    "        'use_default_param_dist': True,\n",
    "        'param_dist': None,\n",
    "        'test_size': 0.2\n",
    "    }\n",
    "    \n",
    "    # Fit data\n",
    "    optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "    \n",
    "    # Save \n",
    "    optimized_pipelines[estimator] = optimized_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "format_str = '{0:<22} {1:<15} {2:<15}'\n",
    "\n",
    "print format_str.format(*['model','train score','test score'])\n",
    "print format_str.format(*['','',''])\n",
    "for x in [[key,value.train_score_,value.test_score_] for key,value in optimized_pipelines.iteritems()]:\n",
    "    print format_str.format(*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest performed the best with a test score of ~0.854.\n",
    "\n",
    "Lets look at the report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print optimized_pipelines['random_forest']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal parameter was 96 for the n_estimators parameter for the RandomizedForestClassifier.\n",
    "\n",
    "### All models with standard scaling\n",
    "We can set the scaling type using the scale_type keyword argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reload(dsl)\n",
    "\n",
    "classifiers = ['knn','logistic_regression','svm',\n",
    "               'multilayer_perceptron','random_forest','adaboost']\n",
    "\n",
    "prefix = 'scale'\n",
    "\n",
    "for estimator in classifiers:\n",
    "    # Set pipeline keyword arguments\n",
    "    optimized_pipeline_kwargs = {\n",
    "        'feature_selection_type': None,\n",
    "        'scale_type': 'standard',\n",
    "        'feature_interactions': False,\n",
    "        'transform_type': None\n",
    "        }\n",
    "\n",
    "    # Initialize pipeline\n",
    "    optimized_pipeline = dsl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "    \n",
    "    # Set pipeline fitting parameters\n",
    "    fit_kwargs = {\n",
    "        'cv': 10,\n",
    "        'num_parameter_combos': None,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 6,\n",
    "        'suppress_output': True,\n",
    "        'use_default_param_dist': True,\n",
    "        'param_dist': None,\n",
    "        'test_size': 0.2\n",
    "    }\n",
    "    \n",
    "    # Fit data\n",
    "    optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "    \n",
    "    # Form name used to save optimized pipeline\n",
    "    pipeline_name = '%s_%s'%(prefix,estimator)\n",
    "    \n",
    "    # Save \n",
    "    optimized_pipelines[pipeline_name] = optimized_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "format_str = '{0:<30} {1:<15} {2:<15}'\n",
    "\n",
    "print format_str.format(*['model','train score','test score'])\n",
    "print format_str.format(*['','',''])\n",
    "for x in [[key,value.train_score_,value.test_score_] for key,value in optimized_pipelines.iteritems()]:\n",
    "    print format_str.format(*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest without scaling still appears to have the best test score. Though that with scaling had closer test and train scores.\n",
    "\n",
    "### All models with SelectKBest feature selection\n",
    "Setting the feature_selection_type keyword argument will use SelectKBest with f_classif for feature selection: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reload(dsl)\n",
    "\n",
    "classifiers = ['knn','logistic_regression','svm',\n",
    "               'multilayer_perceptron','random_forest','adaboost']\n",
    "\n",
    "prefix = 'select'\n",
    "\n",
    "for estimator in classifiers:\n",
    "    # Set pipeline keyword arguments\n",
    "    optimized_pipeline_kwargs = {\n",
    "        'feature_selection_type': 'select_k_best',\n",
    "        'scale_type': None,\n",
    "        'feature_interactions': False,\n",
    "        'transform_type': None\n",
    "        }\n",
    "\n",
    "    # Initialize pipeline\n",
    "    optimized_pipeline = dsl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "    \n",
    "    # Set pipeline fitting parameters\n",
    "    fit_kwargs = {\n",
    "        'cv': 10,\n",
    "        'num_parameter_combos': None,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 6,\n",
    "        'suppress_output': True,\n",
    "        'use_default_param_dist': True,\n",
    "        'param_dist': None,\n",
    "        'test_size': 0.2\n",
    "    }\n",
    "    \n",
    "    # Fit data\n",
    "    optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "    \n",
    "    # Form name used to save optimized pipeline\n",
    "    pipeline_name = '%s_%s'%(prefix,estimator)\n",
    "    \n",
    "    # Save \n",
    "    optimized_pipelines[pipeline_name] = optimized_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "format_str = '{0:<30} {1:<15} {2:<15} {3:<15}'\n",
    "\n",
    "print format_str.format(*['model','train score','test score','train-test'])\n",
    "print format_str.format(*['','','',''])\n",
    "for x in [[key,value.train_score_,value.test_score_,value.train_score_-value.test_score_] for key,value in optimized_pipelines.iteritems()]:\n",
    "    print format_str.format(*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, random_forest performs the best. \n",
    "\n",
    "Though K-nearest neighbors appears to have the smallest difference between testing and training sets.\n",
    "\n",
    "### All models with feature interaction\n",
    "Setting the feature_interactions keyword argument to True will cause the use of feature interactions. The default is to only consider pairwise products, though this be set to higher by overriding using param_dist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reload(dsl)\n",
    "\n",
    "classifiers = ['knn','logistic_regression','svm','multilayer_perceptron','random_forest','adaboost']\n",
    "\n",
    "prefix = 'interact'\n",
    "\n",
    "for estimator in classifiers:\n",
    "    # Set pipeline keyword arguments\n",
    "    optimized_pipeline_kwargs = {\n",
    "        'feature_selection_type': None,\n",
    "        'scale_type': None,\n",
    "        'feature_interactions': True,\n",
    "        'transform_type': None\n",
    "        }\n",
    "\n",
    "    # Initialize pipeline\n",
    "    optimized_pipeline = dsl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "    \n",
    "    # Set pipeline fitting parameters\n",
    "    fit_kwargs = {\n",
    "        'cv': 10,\n",
    "        'num_parameter_combos': None,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 6,\n",
    "        'suppress_output': True,\n",
    "        'use_default_param_dist': True,\n",
    "        'param_dist': None,\n",
    "        'test_size': 0.2\n",
    "    }\n",
    "    \n",
    "    # Fit data\n",
    "    optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "    \n",
    "    # Form name used to save optimized pipeline\n",
    "    pipeline_name = '%s_%s'%(prefix,estimator)\n",
    "    \n",
    "    # Save \n",
    "    optimized_pipelines[pipeline_name] = optimized_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "format_str = '{0:<30} {1:<15} {2:<15} {3:<15}'\n",
    "\n",
    "print format_str.format(*['model','train score','test score','train-test'])\n",
    "print format_str.format(*['','','',''])\n",
    "for x in [[key,value.train_score_,value.test_score_,value.train_score_-value.test_score_] \\\n",
    "          for key,value in optimized_pipelines.iteritems()]:\n",
    "    print format_str.format(*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't appear to result in many gains in this case.\n",
    "\n",
    "### All models with transformed data\n",
    "Setting the transform_type to 'pca' or 't-sne' will apply Principal Component Analysis or t-distributed stochastic neighbor embedding, respectively, to the data before applying the estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reload(dsl)\n",
    "\n",
    "classifiers = ['knn','logistic_regression','svm',\n",
    "               'multilayer_perceptron','random_forest','adaboost']\n",
    "\n",
    "prefix = 'pca'\n",
    "\n",
    "for estimator in classifiers:\n",
    "    # Set pipeline keyword arguments\n",
    "    optimized_pipeline_kwargs = {\n",
    "        'feature_selection_type': None,\n",
    "        'scale_type': None,\n",
    "        'feature_interactions': None,\n",
    "        'transform_type': 'pca'\n",
    "        }\n",
    "\n",
    "    # Initialize pipeline\n",
    "    optimized_pipeline = dsl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "    \n",
    "    # Set pipeline fitting parameters\n",
    "    fit_kwargs = {\n",
    "        'cv': 10,\n",
    "        'num_parameter_combos': None,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 6,\n",
    "        'suppress_output': True,\n",
    "        'use_default_param_dist': True,\n",
    "        'param_dist': None,\n",
    "        'test_size': 0.2\n",
    "    }\n",
    "    \n",
    "    # Fit data\n",
    "    optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "    \n",
    "    # Form name used to save optimized pipeline\n",
    "    pipeline_name = '%s_%s'%(prefix,estimator)\n",
    "    \n",
    "    # Save \n",
    "    optimized_pipelines[pipeline_name] = optimized_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "format_str = '{0:<30} {1:<15} {2:<15} {3:<15}'\n",
    "\n",
    "print format_str.format(*['model','train score','test score','train-test'])\n",
    "print format_str.format(*['','','',''])\n",
    "for x in [[key,value.train_score_,value.test_score_,value.train_score_-value.test_score_] for key,value in optimized_pipelines.iteritems()]:\n",
    "    print format_str.format(*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the use of t-SNE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reload(dsl)\n",
    "\n",
    "classifiers = ['knn','logistic_regression','svm','multilayer_perceptron','random_forest','adaboost']\n",
    "\n",
    "prefix = 't_sne'\n",
    "\n",
    "for estimator in classifiers:\n",
    "    # Set pipeline keyword arguments\n",
    "    optimized_pipeline_kwargs = {\n",
    "        'feature_selection_type': None,\n",
    "        'scale_type': None,\n",
    "        'feature_interactions': None,\n",
    "        'transform_type': 't-sne'\n",
    "        }\n",
    "\n",
    "    # Initialize pipeline\n",
    "    optimized_pipeline = dsl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "    \n",
    "    # Set pipeline fitting parameters\n",
    "    fit_kwargs = {\n",
    "        'cv': 10,\n",
    "        'num_parameter_combos': None,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 6,\n",
    "        'suppress_output': True,\n",
    "        'use_default_param_dist': True,\n",
    "        'param_dist': None,\n",
    "        'test_size': 0.2\n",
    "    }\n",
    "    \n",
    "    # Fit data\n",
    "    optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "    \n",
    "    # Form name used to save optimized pipeline\n",
    "    pipeline_name = '%s_%s'%(prefix,estimator)\n",
    "    \n",
    "    # Save \n",
    "    optimized_pipelines[pipeline_name] = optimized_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "format_str = '{0:<30} {1:<15} {2:<15} {3:<15}'\n",
    "\n",
    "print format_str.format(*['model','train score','test score','train-test'])\n",
    "print format_str.format(*['','','',''])\n",
    "for x in [[key,value.train_score_,value.test_score_,value.train_score_-value.test_score_] for key,value in optimized_pipelines.iteritems()]:\n",
    "    print format_str.format(*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that took forever.\n",
    "\n",
    "We can get a better idea on how long this will take by setting the num_parameter_combos keyword argument. Setting this will only allow that number of grid combinations to be used for each run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reload(dsl)\n",
    "\n",
    "classifiers = ['knn','logistic_regression','svm',\n",
    "               'multilayer_perceptron','random_forest','adaboost']\n",
    "\n",
    "prefix = 't_sne_less_combo'\n",
    "\n",
    "for estimator in classifiers:\n",
    "    # Set pipeline keyword arguments\n",
    "    optimized_pipeline_kwargs = {\n",
    "        'feature_selection_type': None,\n",
    "        'scale_type': None,\n",
    "        'feature_interactions': None,\n",
    "        'transform_type': 't-sne'\n",
    "        }\n",
    "\n",
    "    # Initialize pipeline\n",
    "    optimized_pipeline = dsl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "    \n",
    "    # Set pipeline fitting parameters\n",
    "    fit_kwargs = {\n",
    "        'cv': 10,\n",
    "        'num_parameter_combos': 1,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 6,\n",
    "        'suppress_output': True,\n",
    "        'use_default_param_dist': True,\n",
    "        'param_dist': None,\n",
    "        'test_size': 0.2\n",
    "    }\n",
    "    \n",
    "    # Fit data\n",
    "    optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "    \n",
    "    # Form name used to save optimized pipeline\n",
    "    pipeline_name = '%s_%s'%(prefix,estimator)\n",
    "    \n",
    "    # Save \n",
    "    optimized_pipelines[pipeline_name] = optimized_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying t-sne to the data and then testing the 6 classifiers takes about 7 min. This could be optimized by pre-transforming the data once and then applying the classifiers. I'm thinking of creating some sort of container class that should be able to optimize this in the future.\n",
    "\n",
    "### SelectKBest, standard scaling, and all classifiers\n",
    "Finally, here we appply feature selection and standard scaling for all 6 classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reload(dsl)\n",
    "\n",
    "classifiers = ['knn','logistic_regression','svm',\n",
    "               'multilayer_perceptron','random_forest','adaboost']\n",
    "\n",
    "prefix = 'select_standard'\n",
    "\n",
    "for estimator in classifiers:\n",
    "    # Set pipeline keyword arguments\n",
    "    optimized_pipeline_kwargs = {\n",
    "        'feature_selection_type': 'select_k_best',\n",
    "        'scale_type': 'standard',\n",
    "        'feature_interactions': None,\n",
    "        'transform_type': None\n",
    "        }\n",
    "\n",
    "    # Initialize pipeline\n",
    "    optimized_pipeline = dsl.OptimizedPipeline(estimator,**optimized_pipeline_kwargs)\n",
    "    \n",
    "    # Set pipeline fitting parameters\n",
    "    fit_kwargs = {\n",
    "        'cv': 10,\n",
    "        'num_parameter_combos': None,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 6,\n",
    "        'suppress_output': True,\n",
    "        'use_default_param_dist': True,\n",
    "        'param_dist': None,\n",
    "        'test_size': 0.2\n",
    "    }\n",
    "    \n",
    "    # Fit data\n",
    "    optimized_pipeline.fit(X,y,**fit_kwargs)\n",
    "    \n",
    "    # Form name used to save optimized pipeline\n",
    "    pipeline_name = '%s_%s'%(prefix,estimator)\n",
    "    \n",
    "    # Save \n",
    "    optimized_pipelines[pipeline_name] = optimized_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "format_str = '{0:<40} {1:<15} {2:<15} {3:<15}'\n",
    "\n",
    "print format_str.format(*['model','train score','test score','train-test'])\n",
    "print format_str.format(*['','','',''])\n",
    "for x in [[key,value.train_score_,value.test_score_,value.train_score_-value.test_score_] for key,value in optimized_pipelines.iteritems()]:\n",
    "    print format_str.format(*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(optimized_pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 48 different pre-processing/transformation/classification combinations, this has become rather unwieldy. \n",
    "\n",
    "Here I make a quick dataframe of the test/train scores and visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "model_indices = optimized_pipelines.keys()\n",
    "train_scores = [value.train_score_ for key,value in optimized_pipelines.iteritems()]\n",
    "test_scores = [value.test_score_ for key,value in optimized_pipelines.iteritems()]\n",
    "\n",
    "score_df = pd.DataFrame({'training_score':train_scores,'test_score':test_scores},\n",
    "                        index=model_indices)\n",
    "\n",
    "score_df['test-train'] = score_df['test_score']-score_df['training_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_df['test_score'].sort_values().plot(kind='barh',figsize=(10,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best training score was acheived by the random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_df['test-train'].sort_values().plot(kind='barh',figsize=(10,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = score_df.plot(x=['test_score'],y='test-train',style='o',legend=None)\n",
    "\n",
    "ax.set_xlabel('test score')\n",
    "ax.set_ylabel('test-train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the best model was random forest.\n",
    "\n",
    "Here's the report for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print optimized_pipelines['random_forest']"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
